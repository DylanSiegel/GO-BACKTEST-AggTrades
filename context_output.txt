[{
	"resource": "/Z:/Benchmarks/OOS.go",
	"owner": "_generated_diagnostic_collection_name_#7",
	"code": {
		"value": "default",
		"target": {
			"$mid": 1,
			"path": "/golang.org/x/tools/go/analysis/passes/printf",
			"scheme": "https",
			"authority": "pkg.go.dev"
		}
	},
	"severity": 4,
	"message": "fmt.Fprintf format %s reads arg #19, but call has 18 args",
	"source": "printf",
	"startLineNumber": 220,
	"startColumn": 110,
	"endLineNumber": 220,
	"endColumn": 112,
	"origin": "extHost1"
}]

--- File Tree Structure ---
|-- data/
|-- OOS.go
|-- common.go
|-- data.go
|-- main.go
|-- metrics.go
|-- ofi.go
|-- sanity.go
|-- sum.go
    |-- BTCUSDT/
        |-- 2020/
        |-- 2021/
        |-- 2022/
        |-- 2023/
        |-- 2024/
        |-- 2025/
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..11]/ (11 month dirs)

// --- File: OOS.go ---

```go
package main

import (
	"encoding/json"
	"fmt"
	"os"
	"path/filepath"
	"sort"
	"strings"
	"text/tabwriter"
	"time"
)

// OOSStartDate defines the boundary between IS and OOS.
// All days < OOSStartDate are IS; >= OOSStartDate are OOS.
const OOSStartDate = "2024-01-01"

// ofiOOSAgg aggregates IS/OOS stats per variant.
type ofiOOSAgg struct {
	Variant string

	ISDays   int
	OOSDays  int
	ISBars   int64
	OOSBars  int64
	StartIS  string
	EndIS    string
	StartOOS string
	EndOOS   string

	IS_SumIC      float64
	OOS_SumIC     float64
	IS_SumHAC     float64
	OOS_SumHAC    float64
	IS_SumSR      float64
	OOS_SumSR     float64
	IS_SumProbSR  float64
	OOS_SumProbSR float64
	IS_SumBE      float64
	OOS_SumBE     float64
	IS_SumHit     float64
	OOS_SumHit    float64
	IS_SumFill    float64
	OOS_SumFill   float64
}

// runOOS reads the full OFI metrics report and produces a strict
// IS vs OOS comparison per variant at the configured TargetHz.
func runOOS() {
	// Parse OOS boundary once
	oosBoundary, err := time.Parse("2006-01-02", OOSStartDate)
	if err != nil {
		fmt.Printf("Invalid OOSStartDate const: %v\n", err)
		return
	}

	path := filepath.FromSlash(OFIReportPath)
	f, err := os.Open(path)
	if err != nil {
		fmt.Printf("Error opening OFI report: %v\n", err)
		return
	}
	defer f.Close()

	startT := time.Now()

	var entries []AlphaMetrics
	dec := json.NewDecoder(f)
	if err := dec.Decode(&entries); err != nil {
		fmt.Printf("Error decoding OFI report JSON: %v\n", err)
		return
	}
	if len(entries) == 0 {
		fmt.Println("No OFI metrics found.")
		return
	}

	aggMap := make(map[string]*ofiOOSAgg)

	for _, e := range entries {
		// label format: "SYMBOL|VARIANT|YYYY-MM-DD"
		parts := strings.Split(e.Label, "|")
		if len(parts) < 3 {
			continue
		}
		variant := parts[1]
		dateStr := parts[2]

		// parse date
		dayT, err := time.Parse("2006-01-02", dateStr)
		if err != nil {
			continue
		}
		h, ok := e.Horizon[TargetHz]
		if !ok {
			continue
		}

		a, ok := aggMap[variant]
		if !ok {
			a = &ofiOOSAgg{Variant: variant}
			aggMap[variant] = a
		}

		isOOS := !dayT.Before(oosBoundary)

		if isOOS {
			a.OOSDays++
			a.OOSBars += int64(e.NBars)

			if a.StartOOS == "" || dateStr < a.StartOOS {
				a.StartOOS = dateStr
			}
			if a.EndOOS == "" || dateStr > a.EndOOS {
				a.EndOOS = dateStr
			}

			a.OOS_SumIC += h.ICPearson
			a.OOS_SumHAC += h.HACSharpe
			a.OOS_SumSR += h.TheoreticalSharpe
			a.OOS_SumProbSR += h.ProbSharpeRatio
			a.OOS_SumBE += h.BreakevenBps
			a.OOS_SumHit += h.DirectionalHit
			a.OOS_SumFill += h.FillRate
		} else {
			a.ISDays++
			a.ISBars += int64(e.NBars)

			if a.StartIS == "" || dateStr < a.StartIS {
				a.StartIS = dateStr
			}
			if a.EndIS == "" || dateStr > a.EndIS {
				a.EndIS = dateStr
			}

			a.IS_SumIC += h.ICPearson
			a.IS_SumHAC += h.HACSharpe
			a.IS_SumSR += h.TheoreticalSharpe
			a.IS_SumProbSR += h.ProbSharpeRatio
			a.IS_SumBE += h.BreakevenBps
			a.IS_SumHit += h.DirectionalHit
			a.IS_SumFill += h.FillRate
		}
	}

	if len(aggMap) == 0 {
		fmt.Println("No valid OFI metrics at target horizon for IS/OOS.")
		return
	}

	// Flatten and sort by OOS mean HAC Sharpe descending
	var aggs []*ofiOOSAgg
	for _, a := range aggMap {
		// Only keep variants that have at least some OOS coverage
		if a.OOSDays > 0 {
			aggs = append(aggs, a)
		}
	}
	if len(aggs) == 0 {
		fmt.Println("No variants have out-of-sample days after OOSStartDate.")
		return
	}

	sort.Slice(aggs, func(i, j int) bool {
		di := float64(aggs[i].OOSDays)
		dj := float64(aggs[j].OOSDays)
		if di == 0 || dj == 0 {
			return aggs[i].Variant < aggs[j].Variant
		}
		return (aggs[i].OOS_SumHAC / di) > (aggs[j].OOS_SumHAC / dj)
	})

	// Print summary
	fmt.Printf("\n--- OFI IS/OOS SUMMARY | %s | Horizon: %s ticks ---\n",
		filepath.Base(path), TargetHz)
	fmt.Printf("OOS boundary: %s (IS: < %s, OOS: >= %s)\n", OOSStartDate, OOSStartDate, OOSStartDate)
	fmt.Printf("Processing Time: %s\n\n", time.Since(startT))

	w := tabwriter.NewWriter(os.Stdout, 0, 0, 2, ' ', 0)

	// Header
	fmt.Fprintf(w, "VARIANT\tIS_DAYS\tOOS_DAYS\tIS_HAC\tOOS_HAC\tOOS/IS\tIS_IC\tOOS_IC\tIS_ProbSR\tOOS_ProbSR\tIS_BE_BPS\tOOS_BE_BPS\tIS_HIT\tOOS_HIT\tIS_FILL\tOOS_FILL\tIS_PERIOD\tOOS_PERIOD\n")
	fmt.Fprintf(w, "-------\t-------\t--------\t------\t-------\t------\t-----\t------\t---------\t----------\t----------\t-----------\t------\t-------\t-------\t--------\t---------\t----------\n")

	for _, a := range aggs {
		isN := float64(a.ISDays)
		oosN := float64(a.OOSDays)

		var (
			isHAC, oosHAC, ratio float64
			isIC, oosIC          float64
			isProb, oosProb      float64
			isBE, oosBE          float64
			isHit, oosHit        float64
			isFill, oosFill      float64
		)

		if isN > 0 {
			isHAC = a.IS_SumHAC / isN
			isIC = a.IS_SumIC / isN
			isProb = a.IS_SumProbSR / isN
			isBE = a.IS_SumBE / isN
			isHit = (a.IS_SumHit / isN) * 100.0
			isFill = (a.IS_SumFill / isN) * 100.0
		}
		if oosN > 0 {
			oosHAC = a.OOS_SumHAC / oosN
			oosIC = a.OOS_SumIC / oosN
			oosProb = a.OOS_SumProbSR / oosN
			oosBE = a.OOS_SumBE / oosN
			oosHit = (a.OOS_SumHit / oosN) * 100.0
			oosFill = (a.OOS_SumFill / oosN) * 100.0
		}
		if isHAC != 0 {
			ratio = oosHAC / isHAC
		}

		fmt.Fprintf(
			w,
			"%s\t%d\t%d\t%.2f\t%.2f\t%.2f\t%.4f\t%.4f\t%.2f\t%.2f\t%.2f\t%.2f\t%.1f%%\t%.1f%%\t%.1f%%\t%.1f%%\t%s→%s\t%s→%s\n",
			a.Variant,
			a.ISDays,
			a.OOSDays,
			isHAC,
			oosHAC,
			ratio,
			isIC,
			oosIC,
			isProb,
			oosProb,
			isBE,
			oosBE,
			isHit,
			oosHit,
			isFill,
			oosFill,
			safePeriod(a.StartIS, a.EndIS),
			safePeriod(a.StartOOS, a.EndOOS),
		)
	}

	w.Flush()
	fmt.Println("")
}

// safePeriod formats "start→end" for date ranges, handling empty strings.
func safePeriod(start, end string) string {
	if start == "" && end == "" {
		return ""
	}
	if start == "" {
		return end
	}
	if end == "" {
		return start
	}
	return start + "→" + end
}
```

// --- End File: OOS.go ---

// --- File: common.go ---

```go
package main

import (
	"encoding/binary"
	"math"
)

// --- Shared Configuration ---
const (
	// Hardware Optimization: Ryzen 9 7900X (12 Cores / 24 Threads)
	// We saturate all logical cores for parallel workloads.
	CPUThreads = 24

	Symbol  = "BTCUSDT"
	BaseDir = "data"

	// Default Targets
	TargetYear  = 2024
	TargetMonth = 1

	// Binary Format Constants
	// Floating point scalar for integer compression (1.00 = 100000000)
	PxScale = 100_000_000.0
	QtScale = 100_000_000.0

	// Magic headers for file identification
	AggMagic   = "AGG3"
	IdxMagic   = "QIDX"
	IdxVersion = 1

	// Binary Layout Sizes (Bytes)
	HeaderSize  = 48
	RowSize     = 48
	FeatureSize = 24
)

// --- Binary Structures ---

// AggHeader matches the binary layout for our compressed data files.
// Used by data.go for writing and build.go for reading.
type AggHeader struct {
	Magic    [4]byte
	Version  uint8
	Day      uint8
	ZLevel   uint16
	RowCount uint64
	MinTs    int64
	MaxTs    int64
	Padding  [16]byte
}

// --- Math Helpers ---
// Highly optimized, inlined-capable math functions used across the monolith.

// Mean calculates the arithmetic average of a slice.
func Mean(vals []float64) float64 {
	if len(vals) == 0 {
		return 0.0
	}
	sum := 0.0
	for _, v := range vals {
		sum += v
	}
	return sum / float64(len(vals))
}

// StdDev calculates sample standard deviation.
// Requires pre-calculated mean to avoid redundant iteration in tight loops.
func StdDev(vals []float64, mean float64) float64 {
	if len(vals) < 2 {
		return 0.0
	}
	sumSq := 0.0
	for _, v := range vals {
		d := v - mean
		sumSq += d * d
	}
	return math.Sqrt(sumSq / float64(len(vals)-1))
}

// Correlation calculates Pearson correlation coefficient (r).
func Correlation(x, y []float64) float64 {
	n := len(x)
	if n != len(y) || n == 0 {
		return 0.0
	}
	mx, my := Mean(x), Mean(y)
	sxx, syy, sxy := 0.0, 0.0, 0.0
	for i := 0; i < n; i++ {
		dx := x[i] - mx
		dy := y[i] - my
		sxx += dx * dx
		syy += dy * dy
		sxy += dx * dy
	}
	if sxx == 0 || syy == 0 {
		return 0.0
	}
	return sxy / math.Sqrt(sxx*syy)
}

// --- Binary Helpers ---

// PutRow efficiently packs standard AggTrade data into a byte slice.
// Uses LittleEndian to match x86_64 architecture (Ryzen 7900X) natively.
func PutRow(buf []byte, tid, px, qty, fid uint64, cnt uint32, flags uint16, ts int64) {
	binary.LittleEndian.PutUint64(buf[0:], tid)
	binary.LittleEndian.PutUint64(buf[8:], px)
	binary.LittleEndian.PutUint64(buf[16:], qty)
	binary.LittleEndian.PutUint64(buf[24:], fid)
	binary.LittleEndian.PutUint32(buf[32:], cnt)
	binary.LittleEndian.PutUint16(buf[36:], flags)
	binary.LittleEndian.PutUint64(buf[38:], uint64(ts))
}
```

// --- End File: common.go ---

// --- File: data.go ---

```go
package main

import (
	"archive/zip"
	"bytes"
	"compress/zlib"
	"crypto/sha256"
	"encoding/binary"
	"fmt"
	"io"
	"math"
	"net/http"
	"os"
	"os/signal"
	"path/filepath"
	"strings"
	"sync"
	"syscall"
	"time"
)

// --- Local Constants (Specific to Data Downloader) ---
const (
	// Data Source
	HostData   = "data.binance.vision"
	S3Prefix   = "data/futures/um"
	DataSet    = "aggTrades"
	FallbackDt = "2020-01-01"
)

// --- Globals ---
var (
	httpClient *http.Client
	stopEvent  bool
	stopMu     sync.Mutex

	// Directory Locks to fix race conditions on monthly files
	dirLocks sync.Map
)

func init() {
	// High-throughput Transport (no TLS hacks)
	tr := &http.Transport{
		MaxIdleConns:        100,
		MaxIdleConnsPerHost: 100,
		IdleConnTimeout:     90 * time.Second,
	}
	httpClient = &http.Client{
		Transport: tr,
		Timeout:   20 * time.Second,
	}
}

// runData is called by main()
func runData() {
	// Graceful Shutdown
	sigChan := make(chan os.Signal, 1)
	signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)
	go func() {
		<-sigChan
		stopMu.Lock()
		stopEvent = true
		stopMu.Unlock()
		fmt.Println("\n[warn] Stopping gracefully (finish current jobs)...")
	}()

	fmt.Printf("--- data.go (Ryzen 7900X Optimized) | Symbol: %s ---\n", Symbol)

	start, err := time.Parse("2006-01-02", FallbackDt)
	if err != nil {
		fmt.Printf("[fatal] invalid FallbackDt %q: %v\n", FallbackDt, err)
		return
	}

	end := time.Now().UTC().AddDate(0, 0, -1)

	if end.Before(start) {
		fmt.Println("Nothing to do.")
		return
	}

	// Generate Job Queue
	var days []time.Time
	for d := start; !d.After(end); d = d.AddDate(0, 0, 1) {
		days = append(days, d)
	}

	fmt.Printf("[job] Processing %d days using %d threads.\n", len(days), CPUThreads)

	jobs := make(chan time.Time, len(days))
	results := make(chan string, len(days))
	var wg sync.WaitGroup

	// Workers
	for i := 0; i < CPUThreads; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			for d := range jobs {
				// Check Stop Signal
				stopMu.Lock()
				if stopEvent {
					stopMu.Unlock()
					return
				}
				stopMu.Unlock()

				results <- processDay(d)
			}
		}()
	}

	for _, d := range days {
		jobs <- d
	}
	close(jobs)
	wg.Wait()
	close(results)

	// Stats
	stats := make(map[string]int)
	for r := range results {
		key := strings.SplitN(r, " ", 2)[0]
		stats[key]++
	}
	fmt.Printf("\n[done] %v\n", stats)
}

func processDay(d time.Time) string {
	y, m, day := d.Year(), int(d.Month()), d.Day()

	// Paths
	dirPath := filepath.Join(BaseDir, Symbol, fmt.Sprintf("%04d", y), fmt.Sprintf("%02d", m))
	idxPath := filepath.Join(dirPath, "index.quantdev")
	dataPath := filepath.Join(dirPath, "data.quantdev")

	// 1. Get Directory Lock (serialize Index/Data per month)
	muAny, _ := dirLocks.LoadOrStore(dirPath, &sync.Mutex{})
	mu := muAny.(*sync.Mutex)

	// 2. Check Index (Fast Read)
	mu.Lock()
	indexed := isIndexed(idxPath, day)
	mu.Unlock()

	if indexed {
		return "skip"
	}

	// 3. Download (Concurrent / Slow IO)
	url := fmt.Sprintf("https://%s/%s/daily/%s/%s/%s-%s-%04d-%02d-%02d.zip",
		HostData, S3Prefix, DataSet, Symbol, Symbol, DataSet, y, m, day)

	zipBytes, err := download(url)
	if err != nil {
		if err == errNotFound {
			return "missing"
		}
		return "error_dl"
	}

	// 4. Fast Parse (Concurrent / High CPU)
	aggBlob, count, err := fastZipToAgg3(d, zipBytes)
	if err != nil {
		return "error_parse"
	}
	if count == 0 {
		return "empty"
	}

	// 5. Compress (Concurrent / High CPU)
	var b bytes.Buffer
	const zLevel = zlib.BestSpeed
	w, err := zlib.NewWriterLevel(&b, zLevel)
	if err != nil {
		return "error_zlib"
	}
	if _, err := w.Write(aggBlob); err != nil {
		w.Close()
		return "error_zlib_write"
	}
	if err := w.Close(); err != nil {
		return "error_zlib_close"
	}
	compBlob := b.Bytes()

	// 6. Checksum (Concurrent)
	sum := sha256.Sum256(aggBlob)
	cSum := binary.LittleEndian.Uint64(sum[:8])

	// 7. Write Data & Update Index (Serialized / Fast IO)
	mu.Lock()
	defer mu.Unlock()

	// Double check index in case another thread finished this day while we were processing
	if isIndexed(idxPath, day) {
		return "skip_race"
	}

	if err := os.MkdirAll(dirPath, 0755); err != nil {
		return "error_mkdir"
	}

	// Append Data
	fData, err := os.OpenFile(dataPath, os.O_CREATE|os.O_APPEND|os.O_WRONLY, 0644)
	if err != nil {
		return "error_io"
	}
	stat, err := fData.Stat()
	if err != nil {
		fData.Close()
		return "error_stat"
	}
	offset := stat.Size()

	if _, err := fData.Write(compBlob); err != nil {
		fData.Close()
		return "error_write"
	}
	if err := fData.Close(); err != nil {
		return "error_close_data"
	}

	// Append / Init Index
	fIdx, err := os.OpenFile(idxPath, os.O_CREATE|os.O_RDWR, 0644)
	if err != nil {
		return "error_idx"
	}
	defer fIdx.Close()

	idxStat, err := fIdx.Stat()
	if err != nil {
		return "error_idx_stat"
	}
	if idxStat.Size() == 0 {
		// Init Index Header
		hdr := make([]byte, 16)
		copy(hdr[0:], IdxMagic)
		binary.LittleEndian.PutUint32(hdr[4:], uint32(IdxVersion))
		binary.LittleEndian.PutUint64(hdr[8:], 0) // Count = 0
		if _, err := fIdx.Write(hdr); err != nil {
			return "error_idx_hdr"
		}
	}

	// Index Row: Day(2), Offset(8), Length(8), Checksum(8) = 26 bytes
	row := make([]byte, 26)
	binary.LittleEndian.PutUint16(row[0:], uint16(day))
	binary.LittleEndian.PutUint64(row[2:], uint64(offset))
	binary.LittleEndian.PutUint64(row[10:], uint64(len(compBlob)))
	binary.LittleEndian.PutUint64(row[18:], cSum)

	if _, err := fIdx.Seek(0, io.SeekEnd); err != nil {
		return "error_idx_seek"
	}
	if _, err := fIdx.Write(row); err != nil {
		return "error_idx_write"
	}

	// Increment Index Count (Atomic update under lock)
	if _, err := fIdx.Seek(8, io.SeekStart); err != nil {
		return "error_idx_seek"
	}
	var currentCount uint64
	if err := binary.Read(fIdx, binary.LittleEndian, &currentCount); err != nil {
		return "error_idx_read"
	}
	if _, err := fIdx.Seek(8, io.SeekStart); err != nil {
		return "error_idx_seek"
	}
	if err := binary.Write(fIdx, binary.LittleEndian, currentCount+1); err != nil {
		return "error_idx_write"
	}

	return "ok"
}

// --- Helpers ---

var errNotFound = fmt.Errorf("404")

func download(url string) ([]byte, error) {
	// Retry logic optimized for throughput
	for i := 0; i < 3; i++ {
		resp, err := httpClient.Get(url)
		if err == nil {
			if resp != nil {
				if resp.StatusCode == http.StatusOK {
					data, readErr := io.ReadAll(resp.Body)
					resp.Body.Close()
					return data, readErr
				}
				if resp.StatusCode == http.StatusNotFound {
					resp.Body.Close()
					return nil, errNotFound
				}
				resp.Body.Close()
			}
		}
		time.Sleep(time.Duration(i+1) * 100 * time.Millisecond)
	}
	return nil, fmt.Errorf("timeout")
}

func isIndexed(idxPath string, day int) bool {
	f, err := os.Open(idxPath)
	if err != nil {
		return false
	}
	defer f.Close()

	// Read Header
	hdr := make([]byte, 16)
	if _, err := io.ReadFull(f, hdr); err != nil {
		return false
	}
	if string(hdr[:4]) != IdxMagic {
		return false
	}
	count := binary.LittleEndian.Uint64(hdr[8:])

	// Scan Rows (max 31)
	row := make([]byte, 26)
	for i := uint64(0); i < count; i++ {
		if _, err := io.ReadFull(f, row); err != nil {
			break
		}
		if int(binary.LittleEndian.Uint16(row[0:])) == day {
			return true
		}
	}
	return false
}

// fastZipToAgg3: Zero-alloc column scanning + Binary Packing
func fastZipToAgg3(t time.Time, zipData []byte) ([]byte, uint64, error) {
	r, err := zip.NewReader(bytes.NewReader(zipData), int64(len(zipData)))
	if err != nil {
		return nil, 0, err
	}

	const zLevel = zlib.BestSpeed // informational only, used in header

	for _, f := range r.File {
		if !strings.HasSuffix(f.Name, ".csv") {
			continue
		}
		rc, err := f.Open()
		if err != nil {
			continue
		}

		// 32GB RAM allows reading full CSV.
		data, err := io.ReadAll(rc)
		rc.Close()
		if err != nil {
			return nil, 0, err
		}

		// Estimation: ~70 bytes CSV -> 48 bytes Bin
		estRows := len(data) / 50
		if estRows < 1 {
			estRows = 1
		}
		blob := make([]byte, 0, estRows*RowSize)
		rowBuf := make([]byte, RowSize)

		var (
			minTs int64 = math.MaxInt64
			maxTs int64 = math.MinInt64
			count uint64

			// Column Tracking
			// 0:id, 1:px, 2:qty, 3:fid, 4:lid, 5:ts, 6:m
			colIdx int
			start  int
			i      int
			n      = len(data)
		)

		// Skip Header Line
		for i < n {
			if data[i] == '\n' {
				i++
				start = i
				break
			}
			i++
		}

		// State Machine Loop
		for i < n {
			b := data[i]

			switch b {
			case ',':
				colSlice := data[start:i]

				switch colIdx {
				case 0: // AggID
					binary.LittleEndian.PutUint64(rowBuf[0:], fastParseUint(colSlice))
				case 1: // Price
					binary.LittleEndian.PutUint64(rowBuf[8:], fastParseFloatFixed(colSlice))
				case 2: // Qty
					binary.LittleEndian.PutUint64(rowBuf[16:], fastParseFloatFixed(colSlice))
				case 3: // FirstID
					binary.LittleEndian.PutUint64(rowBuf[24:], fastParseUint(colSlice))
				case 4: // LastID -> Count
					fid := binary.LittleEndian.Uint64(rowBuf[24:])
					lid := fastParseUint(colSlice)
					binary.LittleEndian.PutUint32(rowBuf[32:], uint32(lid-fid+1))
				case 5: // Time
					ts := fastParseUint(colSlice)
					binary.LittleEndian.PutUint64(rowBuf[38:], ts)
					if int64(ts) < minTs {
						minTs = int64(ts)
					}
					if int64(ts) > maxTs {
						maxTs = int64(ts)
					}
				}

				colIdx++
				start = i + 1

			case '\n':
				// End of row (Maker Flag)
				colSlice := data[start:i]

				// Case 6: is_buyer_maker
				flags := uint16(0)
				if len(colSlice) > 0 {
					c := colSlice[0]
					if c == 't' || c == 'T' {
						flags = 1
					}
				}
				binary.LittleEndian.PutUint16(rowBuf[36:], flags)

				// Append Row
				blob = append(blob, rowBuf...)
				count++

				colIdx = 0
				start = i + 1
			}
			i++
		}

		// Handle case where file doesn't end with \n
		if colIdx == 6 && start < n {
			colSlice := data[start:n]
			flags := uint16(0)
			if len(colSlice) > 0 && (colSlice[0] == 't' || colSlice[0] == 'T') {
				flags = 1
			}
			binary.LittleEndian.PutUint16(rowBuf[36:], flags)
			blob = append(blob, rowBuf...)
			count++
		}

		if count == 0 {
			return nil, 0, nil
		}

		// Header construction (Matches AggHeader: 48 bytes)
		hdr := make([]byte, HeaderSize)
		copy(hdr[0:], AggMagic)
		hdr[4] = 1
		hdr[5] = uint8(t.Day())
		binary.LittleEndian.PutUint16(hdr[6:], uint16(zLevel)) // zlib level (informational)
		binary.LittleEndian.PutUint64(hdr[8:], count)
		binary.LittleEndian.PutUint64(hdr[16:], uint64(minTs))
		binary.LittleEndian.PutUint64(hdr[24:], uint64(maxTs))
		// Bytes 32-47 are padding (zero initialized by make)

		return append(hdr, blob...), count, nil
	}
	return nil, 0, fmt.Errorf("no csv")
}

// --- High Performance Parsers (No Alloc) ---

func fastParseUint(b []byte) uint64 {
	var n uint64
	for _, c := range b {
		n = n*10 + uint64(c-'0')
	}
	return n
}

// Converts "123.45" -> 12345000000 (scaled 1e8)
func fastParseFloatFixed(b []byte) uint64 {
	var n uint64
	seenDot := false
	decimals := 0

	for _, c := range b {
		if c == '.' {
			seenDot = true
			continue
		}
		n = n*10 + uint64(c-'0')
		if seenDot {
			decimals++
		}
	}

	// Adjust Scale 1e8
	const target = 8
	if decimals < target {
		for i := 0; i < (target - decimals); i++ {
			n *= 10
		}
	} else if decimals > target {
		for i := 0; i < (decimals - target); i++ {
			n /= 10
		}
	}
	return n
}
```

// --- End File: data.go ---

// --- File: main.go ---

```go
package main

import (
	"fmt"
	"os"
	"runtime"
	"time"
)

func main() {
	// Hardware Optimization: Ryzen 9 7900X
	// Force the Go runtime to schedule goroutines across all logical threads.
	runtime.GOMAXPROCS(CPUThreads)

	if len(os.Args) < 2 {
		printHelp()
		os.Exit(1)
	}

	start := time.Now()
	cmd := os.Args[1]

	switch cmd {
	case "data":
		runData() // Download + compress Binance data
	case "ofi":
		runOFI() // Mass-test OFI core directional variants
	case "sum":
		runSum() // Summarize OFI reports
	case "sanity":
		runSanity() // Verify file integrity
	default:
		fmt.Printf("Unknown command: %s\n", cmd)
		printHelp()
		os.Exit(1)
	}

	fmt.Printf("\n[sys] Execution Time: %s | Mem: %s\n", time.Since(start), getMemUsage())
}

func printHelp() {
	fmt.Println("Usage: quant.exe [command]")
	fmt.Println("--- DATA PIPELINE ---")
	fmt.Println("  data    - Download & compress raw aggTrades")
	fmt.Println("--- ALPHA LAB ---")
	fmt.Println("  ofi     - Test OFI core directional variants (15ms lag)")
	fmt.Println("  sum     - Summarize OFI metrics (variant ranking)")
	fmt.Println("--- AUDIT ---")
	fmt.Println("  sanity  - Verify data/index integrity")
}

func getMemUsage() string {
	var m runtime.MemStats
	runtime.ReadMemStats(&m)
	return fmt.Sprintf("%d MB", m.Alloc/1024/1024)
}
```

// --- End File: main.go ---

// --- File: metrics.go ---

```go
package main

import (
	"encoding/json"
	"math"
	"math/rand"
	"os"
	"sort"
)

// --- Top-Level Types --------------------------------------------------------

// AlphaMetrics represents the full metrics set for one signal on one dataset
// (e.g., one day, or one long run across many days).
// Horizons map contains per-horizon performance stats keyed by horizon string
// (e.g. "20", "50", "100").
type AlphaMetrics struct {
	Label   string                    `json:"label"`  // e.g. "BTCUSDT|OFI_VolumeTime_50M|2024-01-01"
	NBars   int                       `json:"n_bars"` // number of samples used
	Signal  SignalMetrics             `json:"signal"` // distribution / quality of the raw signal
	Horizon map[string]HorizonMetrics `json:"horizons"`
}

// SignalMetrics describes properties of the raw alpha signal (no horizon).
type SignalMetrics struct {
	Mean        float64 `json:"mean"`
	StdDev      float64 `json:"std_dev"`
	Skew        float64 `json:"skew"`
	Kurtosis    float64 `json:"kurtosis"`
	PctOutliers float64 `json:"pct_outliers"`
	Autocorr    float64 `json:"autocorr_lag1"`
	Turnover    float64 `json:"turnover_per_bar"`
	ZeroPct     float64 `json:"pct_zero_signal"`
	Hurst       float64 `json:"hurst_exponent"`
	Entropy     float64 `json:"shannon_entropy"`
}

// HorizonMetrics describes predictive and PnL properties for a given
// future-horizon (e.g. +20, +50, +100 ticks).
type HorizonMetrics struct {
	// Predictive Power
	ICPearson         float64 `json:"ic_pearson"`          // Pearson IC
	ICSpearman        float64 `json:"ic_spearman"`         // Rank IC
	ICStd             float64 `json:"ic_std"`              // Std dev of rolling IC
	ICIR              float64 `json:"ic_ir"`               // Information ratio of IC (mean/std of rolling IC)
	ShuffledICPearson float64 `json:"shuffled_ic_pearson"` // IC after shuffling returns (leakage sanity)

	TStat          float64 `json:"t_stat_beta"`     // t-stat of regression beta
	DirectionalHit float64 `json:"directional_hit"` // P(sign(sig) == sign(ret))

	// Linear Relationship
	Alpha float64 `json:"alpha"` // regression intercept (returns space)
	Beta  float64 `json:"beta"`  // regression slope (signal -> returns)

	// Monotonicity / Execution
	DecileSpreadBps float64 `json:"decile_spread_bps"` // top-bottom decile spread
	BreakevenBps    float64 `json:"breakeven_cost_bps"`
	FillRate        float64 `json:"fill_rate_est"`

	// Returns Profile (Sharpe-style)
	TheoreticalSharpe float64 `json:"theoretical_sharpe"`
	HACSharpe         float64 `json:"hac_sharpe"`
	ProbSharpeRatio   float64 `json:"prob_sharpe_ratio"`
	SortinoRatio      float64 `json:"sortino_ratio"`
	CalmarRatio       float64 `json:"calmar_ratio"`

	// Trade Statistics
	WinRate      float64 `json:"win_rate"`
	ProfitFactor float64 `json:"profit_factor"`
	AvgWinLoss   float64 `json:"avg_win_loss_ratio"`

	// Alpha Decay
	AlphaHalfLifeBars float64 `json:"alpha_half_life_bars"` // half-life in bars
	AlphaHalfLifeMs   float64 `json:"alpha_half_life_ms"`   // half-life in milliseconds (filled by caller)
}

// --- Signal / Horizon Metric Calculators -----------------------------------

// ComputeSignalMetrics analyzes the distribution and stability of a raw signal
// (no horizon notion here).
func ComputeSignalMetrics(sig []float64) SignalMetrics {
	s := SignalMetrics{}
	n := len(sig)
	if n == 0 {
		return s
	}

	m := Mean(sig)
	s.Mean = m
	s.StdDev = StdDev(sig, m)

	if s.StdDev == 0 {
		// Degenerate constant signal
		return s
	}

	outliers := 0
	zeros := 0
	sumDiff := 0.0

	sum3 := 0.0
	sum4 := 0.0

	for i, v := range sig {
		if math.Abs((v-m)/s.StdDev) > 3.0 {
			outliers++
		}
		if math.Abs(v) < 1e-9 {
			zeros++
		}
		if i > 0 {
			sumDiff += math.Abs(v - sig[i-1])
		}
		d := (v - m) / s.StdDev
		sum3 += d * d * d
		sum4 += d * d * d * d
	}

	s.PctOutliers = float64(outliers) / float64(n)
	s.Turnover = sumDiff / float64(n)
	s.ZeroPct = float64(zeros) / float64(n)
	s.Skew = sum3 / float64(n)
	s.Kurtosis = (sum4 / float64(n)) - 3.0

	s.Autocorr = AutoCorrelation(sig, 1)
	s.Hurst = EstimateHurst(sig)
	s.Entropy = EstimateEntropy(sig, 50)

	return s
}

// ComputeHorizonMetrics computes all key directional + PnL metrics for a given
// signal and aligned future returns at some horizon.
func ComputeHorizonMetrics(sig, ret []float64) HorizonMetrics {
	h := HorizonMetrics{}
	n := len(sig)
	if n == 0 || n != len(ret) {
		return h
	}
	if n < 200 {
		// Require some minimum sample size for stable stats
		return h
	}

	// 1. Basic correlations / regression
	h.ICPearson = Correlation(sig, ret)
	h.ICSpearman = SpearmanCorrelation(sig, ret)

	alpha, beta := SimpleOLS(sig, ret)
	h.Alpha = alpha
	h.Beta = beta

	// T-Stat on beta
	mx := Mean(sig)
	rss, sx := 0.0, 0.0
	for i := 0; i < n; i++ {
		pred := alpha + beta*sig[i]
		resid := ret[i] - pred
		rss += resid * resid
		d := sig[i] - mx
		sx += d * d
	}
	if sx > 0 {
		stdErr := math.Sqrt(rss / float64(n-2))
		if stdErr > 0 {
			h.TStat = beta / (stdErr / math.Sqrt(sx))
		}
	}

	// 2. Directional hit rate
	h.DirectionalHit = directionalHitRate(sig, ret)

	// 3. Monotonicity via quantile spread
	h.DecileSpreadBps = CalcQuantileSpread(sig, ret, 10) * 10000.0

	// 4. PnL stream & execution-style metrics
	var (
		grossWin, grossLoss   float64
		wins, losses          float64
		totalPnL, turnover    float64
		downsideSq, returnsSq float64
		peakCumPnl, maxDD     float64
		cumPnl                float64
		pnlStream             = make([]float64, n)
		filledCount           = 0.0
	)

	prevSig := 0.0
	for i := 0; i < n; i++ {
		// Fill heuristic: we assume a trade is filled if |ret| > 0.5 bps
		if math.Abs(ret[i]) > 0.00005 {
			filledCount++
		}

		pnl := sig[i] * ret[i]
		pnlStream[i] = pnl
		totalPnL += pnl

		if i > 0 {
			turnover += math.Abs(sig[i] - prevSig)
		}
		prevSig = sig[i]

		if pnl > 0 {
			grossWin += pnl
			wins++
		} else if pnl < 0 {
			grossLoss += math.Abs(pnl)
			losses++
			downsideSq += pnl * pnl
		}

		cumPnl += pnl
		if cumPnl > peakCumPnl {
			peakCumPnl = cumPnl
		}
		if (peakCumPnl - cumPnl) > maxDD {
			maxDD = peakCumPnl - cumPnl
		}
		returnsSq += pnl * pnl
	}

	// Fill rate
	h.FillRate = filledCount / float64(n)

	// Breakeven cost (bps) = PnL per unit turnover
	if turnover > 0 {
		h.BreakevenBps = (totalPnL / turnover) * 10000.0
	}

	// Hit stats
	if (wins + losses) > 0 {
		h.WinRate = wins / (wins + losses)
	}
	if grossLoss > 0 {
		h.ProfitFactor = grossWin / grossLoss
		if wins > 0 {
			avgWin := grossWin / wins
			avgLoss := grossLoss / losses
			if avgLoss > 0 {
				h.AvgWinLoss = avgWin / avgLoss
			}
		}
	} else if grossWin > 0 {
		h.ProfitFactor = 100.0
	}

	// 5. Sharpe / Sortino / Calmar
	meanPnl := totalPnL / float64(n)
	varPnl := (returnsSq / float64(n)) - (meanPnl * meanPnl)
	stdPnl := math.Sqrt(varPnl)

	// barsPerYear: adjust if you change horizon semantics
	const barsPerYear = 288.0 * 365.0 // 5-min-equivalent

	if stdPnl > 0 {
		h.TheoreticalSharpe = (meanPnl / stdPnl) * math.Sqrt(barsPerYear)

		if downsideSq > 0 {
			downsideDev := math.Sqrt(downsideSq / float64(n))
			if downsideDev > 0 {
				h.SortinoRatio = (meanPnl / downsideDev) * math.Sqrt(barsPerYear)
			}
		}
	}

	if maxDD > 0 {
		h.CalmarRatio = totalPnL / maxDD
	}

	// 6. HAC Sharpe & Prob(SR>0)
	rho := AutoCorrelation(pnlStream, 1)
	nwAdj := 1.0
	if math.Abs(rho) < 1.0 {
		nwAdj = math.Sqrt(1.0 - rho*rho)
	}
	h.HACSharpe = h.TheoreticalSharpe * nwAdj

	if stdPnl > 0 {
		skew, kurt := CalcHigherMoments(pnlStream)
		sr := meanPnl / stdPnl
		denominator := math.Sqrt(1.0 - skew*sr + ((kurt-1.0)/4.0)*sr*sr)
		if denominator > 0 {
			zVal := (sr * math.Sqrt(float64(n)-1.0)) / denominator
			h.ProbSharpeRatio = NormalCDF(zVal)
		}
	}

	// 7. Alpha half-life estimation from autocorrelation of signal
	h.AlphaHalfLifeBars = estimateHalfLifeBars(sig)

	// 8. Rolling IC stats for ICIR
	const icWindow = 2000
	meanICw, stdICw := rollingICStats(sig, ret, icWindow)
	if stdICw > 0 {
		h.ICStd = stdICw
		h.ICIR = meanICw / stdICw
	}

	// 9. Permutation IC sanity (anti-leakage check)
	h.ShuffledICPearson = shuffledIC(sig, ret)

	return h
}

// directionalHitRate: probability that signal and return have matching sign.
func directionalHitRate(sig, ret []float64) float64 {
	n := len(sig)
	if n == 0 || n != len(ret) {
		return 0
	}
	hits := 0
	valid := 0
	for i := 0; i < n; i++ {
		s := sig[i]
		r := ret[i]
		if s == 0 || r == 0 {
			continue
		}
		valid++
		if (s > 0 && r > 0) || (s < 0 && r < 0) {
			hits++
		}
	}
	if valid == 0 {
		return 0
	}
	return float64(hits) / float64(valid)
}

// estimateHalfLifeBars computes a half-life in "bars" based on log-ACF vs lag.
func estimateHalfLifeBars(sig []float64) float64 {
	n := len(sig)
	if n < 3 {
		return 0
	}
	lags := []float64{1, 2, 3, 4, 5}
	logAc := make([]float64, len(lags))
	for k := range lags {
		ac := AutoCorrelation(sig, int(lags[k]))
		if ac <= 0 {
			ac = 0.0001
		}
		logAc[k] = math.Log(ac)
	}
	_, slope := SimpleOLS(lags, logAc)
	if slope < 0 {
		// half-life in "bars"
		return -0.693147 / slope
	}
	return 0
}

// rollingICStats computes mean/std of IC over non-overlapping windows.
func rollingICStats(sig, ret []float64, window int) (meanIC, stdIC float64) {
	n := len(sig)
	if n != len(ret) || n < window*2 {
		return 0, 0
	}
	// non-overlapping windows
	var ics []float64
	for i := 0; i+window <= n; i += window {
		ic := Correlation(sig[i:i+window], ret[i:i+window])
		ics = append(ics, ic)
	}
	if len(ics) < 2 {
		return 0, 0
	}
	m := Mean(ics)
	sumSq := 0.0
	for _, v := range ics {
		d := v - m
		sumSq += d * d
	}
	std := math.Sqrt(sumSq / float64(len(ics)-1))
	return m, std
}

// shuffledIC computes IC between signal and a shuffled version of ret.
func shuffledIC(sig, ret []float64) float64 {
	n := len(sig)
	if n == 0 || n != len(ret) {
		return 0
	}
	tmp := make([]float64, n)
	copy(tmp, ret)
	// deterministic seed based on length
	r := rand.New(rand.NewSource(int64(n)*7919 + 1234567))
	for i := n - 1; i > 0; i-- {
		j := r.Intn(i + 1)
		tmp[i], tmp[j] = tmp[j], tmp[i]
	}
	return Correlation(sig, tmp)
}

// --- Advanced Math Helpers --------------------------------------------------

// CalcHigherMoments returns (skew, excess kurtosis) of x.
func CalcHigherMoments(x []float64) (skew, kurt float64) {
	n := float64(len(x))
	if n < 3 {
		return 0, 0
	}
	m := Mean(x)
	s := StdDev(x, m)
	if s == 0 {
		return 0, 0
	}

	sum3, sum4 := 0.0, 0.0
	for _, v := range x {
		d := (v - m) / s
		sum3 += d * d * d
		sum4 += d * d * d * d
	}
	skew = sum3 / n
	kurt = (sum4 / n) - 3.0
	return
}

// NormalCDF is Phi(x) for standard normal.
func NormalCDF(x float64) float64 {
	return 0.5 * (1 + math.Erf(x/math.Sqrt2))
}

// AutoCorrelation uses Pearson Correlation lagged by "lag".
func AutoCorrelation(x []float64, lag int) float64 {
	n := len(x)
	if lag >= n || lag <= 0 {
		return 0
	}
	return Correlation(x[:n-lag], x[lag:])
}

// SimpleOLS fits y = alpha + beta*x using least squares.
func SimpleOLS(x, y []float64) (alpha, beta float64) {
	if len(x) == 0 || len(x) != len(y) {
		return 0, 0
	}
	mx, my := Mean(x), Mean(y)
	num, den := 0.0, 0.0
	for i := 0; i < len(x); i++ {
		dx := x[i] - mx
		num += dx * (y[i] - my)
		den += dx * dx
	}
	if den == 0 {
		return 0, 0
	}
	beta = num / den
	alpha = my - beta*mx
	return
}

type rankPair struct {
	val float64
	idx int
}

// SpearmanCorrelation = Pearson correlation of ranks.
func SpearmanCorrelation(x, y []float64) float64 {
	if len(x) == 0 || len(x) != len(y) {
		return 0
	}
	rx := getRanks(x)
	ry := getRanks(y)
	return Correlation(rx, ry)
}

func getRanks(v []float64) []float64 {
	n := len(v)
	pairs := make([]rankPair, n)
	for i, val := range v {
		pairs[i] = rankPair{val, i}
	}
	sort.Slice(pairs, func(i, j int) bool { return pairs[i].val < pairs[j].val })
	ranks := make([]float64, n)
	for i, p := range pairs {
		ranks[p.idx] = float64(i + 1)
	}
	return ranks
}

// CalcQuantileSpread: top-vs-bottom bucket mean return for a ranking signal.
func CalcQuantileSpread(sig, ret []float64, buckets int) float64 {
	n := len(sig)
	if n == 0 || n != len(ret) || buckets <= 1 {
		return 0
	}
	pairs := make([]rankPair, n)
	for i, val := range sig {
		pairs[i] = rankPair{val, i}
	}
	sort.Slice(pairs, func(i, j int) bool { return pairs[i].val < pairs[j].val })

	sz := n / buckets
	if sz == 0 {
		return 0
	}

	sumBot, sumTop := 0.0, 0.0
	for i := 0; i < sz; i++ {
		sumBot += ret[pairs[i].idx]
	}
	for i := n - sz; i < n; i++ {
		sumTop += ret[pairs[i].idx]
	}
	return (sumTop / float64(sz)) - (sumBot / float64(sz))
}

// EstimateEntropy: Shannon entropy in bits, using fixed binning.
func EstimateEntropy(x []float64, bins int) float64 {
	n := len(x)
	if n == 0 || bins <= 1 {
		return 0
	}
	minV, maxV := x[0], x[0]
	for _, v := range x {
		if v < minV {
			minV = v
		}
		if v > maxV {
			maxV = v
		}
	}
	if minV == maxV {
		return 0
	}
	hist := make([]int, bins)
	rng := maxV - minV
	for _, v := range x {
		idx := int(float64(bins) * (v - minV) / rng)
		if idx < 0 {
			idx = 0
		}
		if idx >= bins {
			idx = bins - 1
		}
		hist[idx]++
	}
	entropy := 0.0
	total := float64(n)
	for _, c := range hist {
		if c > 0 {
			p := float64(c) / total
			entropy -= p * math.Log2(p)
		}
	}
	return entropy
}

// EstimateHurst: simple R/S-based proxy for Hurst exponent over the full series.
func EstimateHurst(x []float64) float64 {
	n := len(x)
	if n < 10 {
		return 0.5
	}
	m := Mean(x)
	cumDev := 0.0
	maxCum, minCum := -1e9, 1e9
	ss := 0.0

	for _, v := range x {
		dev := v - m
		cumDev += dev
		if cumDev > maxCum {
			maxCum = cumDev
		}
		if cumDev < minCum {
			minCum = cumDev
		}
		ss += dev * dev
	}

	std := math.Sqrt(ss / float64(n))
	if std == 0 {
		return 0.5
	}
	rRange := maxCum - minCum
	return math.Log(rRange/std) / math.Log(float64(n))
}

// --- Persistence ------------------------------------------------------------

// SaveAlphaMetrics writes a slice of AlphaMetrics to a JSON file.
func SaveAlphaMetrics(path string, metrics []AlphaMetrics) error {
	if err := os.MkdirAll(filepathDir(path), 0755); err != nil {
		return err
	}
	f, err := os.Create(path)
	if err != nil {
		return err
	}
	defer f.Close()

	enc := json.NewEncoder(f)
	enc.SetIndent("", "  ")
	return enc.Encode(metrics)
}

// filepathDir is a tiny helper to avoid importing filepath just for Dir.
func filepathDir(path string) string {
	last := -1
	for i := len(path) - 1; i >= 0; i-- {
		if path[i] == '/' || path[i] == '\\' {
			last = i
			break
		}
	}
	if last <= 0 {
		return "."
	}
	return path[:last]
}
```

// --- End File: metrics.go ---

// --- File: ofi.go ---

```go
package main

import (
	"bytes"
	"compress/zlib"
	"encoding/binary"
	"fmt"
	"io"
	"math"
	"os"
	"path/filepath"
	"sort"
	"strconv"
	"sync"
	"time"
)

// --- Configuration ----------------------------------------------------------

const (
	OFIExecutionLagMS = 15   // execution/network lag in milliseconds
	OFIWarmupTrades   = 2000 // ignore first N trades for metrics
)

// Horizons to test (in ticks after entry)
var OFIHorizons = []int{20, 50, 100}

// --- Task ID ---------------------------------------------------------------

type ofiTask struct {
	Y, M, D int
}

// --- Variant Configuration --------------------------------------------------

type OFIVariant struct {
	ID           string  // label: "OFI_VolumeTime_50M", etc.
	Kind         string  // "voltime", "invariant", "depth", "cluster", "raw"
	Alpha        float64 // EMA alpha (for non-voltime variants)
	DollarLambda float64 // for VolumeTime variants: exp(-dollar / lambda), 0 if unused
	NThreshold   int     // threshold on n_t (trade count), e.g. 13
	NBoost       float64 // depth / cluster weight multiplier
	UseDepth     bool    // multiply flow by n_t
	ZWindow      int     // rolling z-score window; 0 = off
	ResetEvery   int     // reset OFI every N trades (event-time); 0 = off
}

// Complete list of OFI variants to test.
var ofiVariants = []OFIVariant{
	{
		ID:           "OFI_VolumeTime_50M",
		Kind:         "voltime",
		DollarLambda: 50_000_000,
	},
	{
		ID:           "OFI_VolumeTime_30M",
		Kind:         "voltime",
		DollarLambda: 30_000_000,
	},
	{
		ID:         "OFI_Invariant_2025_Slow",
		Kind:       "invariant",
		Alpha:      0.018,
		NThreshold: 13,
		NBoost:     8.5,
		UseDepth:   true,
	},
	{
		ID:         "OFI_Invariant_2025_Fast",
		Kind:       "invariant",
		Alpha:      0.052,
		NThreshold: 13,
		NBoost:     8.5,
		UseDepth:   true,
	},
	{
		ID:       "OFI_ZScore_DepthWeighted_20k",
		Kind:     "depth",
		Alpha:    0.04,
		UseDepth: true,
		ZWindow:  20_000,
	},
	{
		ID:         "OFI_EventTime_2000",
		Kind:       "depth",
		Alpha:      0.08,
		UseDepth:   true,
		ResetEvery: 2_000,
	},
	{
		ID:       "OFI_DepthWeighted_SlowEMA",
		Kind:     "depth",
		Alpha:    0.015,
		UseDepth: true,
	},
	{
		ID:         "OFI_ClusterBoost_9_5x",
		Kind:       "cluster",
		Alpha:      0.05,
		NThreshold: 13,
		NBoost:     9.5,
	},
	{
		ID:      "OFI_ZScore10k_Raw_Slow",
		Kind:    "raw",
		Alpha:   0.02,
		ZWindow: 10_000,
	},
}

// --- OFI State --------------------------------------------------------------

type ofiState struct {
	cfg *OFIVariant

	ofi        float64
	tradeCount int // for ResetEvery

	// rolling z-score state
	zBuf   []float64
	zHead  int
	zCount int
	zSum   float64
	zSumSq float64
}

func newOFIState(cfg *OFIVariant) *ofiState {
	return &ofiState{
		cfg: cfg,
	}
}

func (s *ofiState) updateZScore(value float64) float64 {
	w := s.cfg.ZWindow
	if w <= 0 {
		return value
	}
	if s.zBuf == nil {
		s.zBuf = make([]float64, w)
	}

	if s.zCount < w {
		// still filling window
		s.zBuf[s.zHead] = value
		s.zSum += value
		s.zSumSq += value * value
		s.zCount++
		s.zHead++
		if s.zHead == w {
			s.zHead = 0
		}
	} else {
		// full window: replace oldest
		old := s.zBuf[s.zHead]
		s.zBuf[s.zHead] = value
		s.zSum += value - old
		s.zSumSq += value*value - old*old
		s.zHead++
		if s.zHead == w {
			s.zHead = 0
		}
	}

	if s.zCount < 2 {
		return 0
	}
	mean := s.zSum / float64(s.zCount)
	variance := s.zSumSq/float64(s.zCount) - mean*mean
	if variance <= 0 {
		return 0
	}
	return (value - mean) / math.Sqrt(variance)
}

// Update computes the OFI signal for a single tick.
func (s *ofiState) Update(px, qty float64, nTrades int, sign float64) float64 {
	cfg := s.cfg

	// base signed flow
	flow := sign * qty
	if cfg.UseDepth {
		flow *= float64(nTrades)
	}

	switch cfg.Kind {
	case "voltime":
		// OFI_t = s_t q_t p_t + exp(- q_t p_t / Lambda) * OFI_{t-1}
		dollar := qty * px
		decay := math.Exp(-dollar / cfg.DollarLambda)
		s.ofi = sign*qty*px + decay*s.ofi

	case "invariant":
		// f_t = s q w(n); w boosted above threshold
		w := float64(nTrades)
		if nTrades >= cfg.NThreshold {
			w *= cfg.NBoost
		}
		f := sign * qty * w
		alpha := cfg.Alpha
		s.ofi = alpha*f + (1.0-alpha)*s.ofi

	case "cluster":
		// w_t = { NBoost if n >= threshold; 1 else }
		w := 1.0
		if nTrades >= cfg.NThreshold {
			w = cfg.NBoost
		}
		f := sign * qty * w
		alpha := cfg.Alpha
		s.ofi = alpha*f + (1.0-alpha)*s.ofi

	case "depth", "raw":
		// f is flow or depth-weighted flow
		alpha := cfg.Alpha
		s.ofi = alpha*flow + (1.0-alpha)*s.ofi

	default:
		// fallback: raw EMA on flow
		alpha := cfg.Alpha
		s.ofi = alpha*flow + (1.0-alpha)*s.ofi
	}

	s.tradeCount++
	if cfg.ResetEvery > 0 && s.tradeCount >= cfg.ResetEvery {
		s.ofi = 0
		s.tradeCount = 0
	}

	value := s.ofi
	if cfg.ZWindow > 0 {
		value = s.updateZScore(value)
	}
	return value
}

// --- Tick Series Loader -----------------------------------------------------

// loadDayTicks loads one day from data.quantdev for (y,m,d) and returns
// SoA arrays: timestamps, prices, qty, sign, nTrades.
//
// sign = +1 for taker buy, -1 for taker sell
func loadDayTicks(y, m, d int) (ts []int64, px, qty, sign []float64, nTrades []int, ok bool) {
	dir := filepath.Join(BaseDir, Symbol, fmt.Sprintf("%04d", y), fmt.Sprintf("%02d", m))
	idxPath := filepath.Join(dir, "index.quantdev")
	dataPath := filepath.Join(dir, "data.quantdev")

	offset, length := findBlobOFI(idxPath, d)
	if length == 0 {
		return nil, nil, nil, nil, nil, false
	}

	f, err := os.Open(dataPath)
	if err != nil {
		return nil, nil, nil, nil, nil, false
	}
	defer f.Close()

	if _, err := f.Seek(int64(offset), io.SeekStart); err != nil {
		return nil, nil, nil, nil, nil, false
	}

	compData := make([]byte, length)
	if _, err := io.ReadFull(f, compData); err != nil {
		return nil, nil, nil, nil, nil, false
	}

	r, err := zlib.NewReader(bytes.NewReader(compData))
	if err != nil {
		return nil, nil, nil, nil, nil, false
	}
	blob, err := io.ReadAll(r)
	r.Close()
	if err != nil {
		return nil, nil, nil, nil, nil, false
	}

	if len(blob) < HeaderSize {
		return nil, nil, nil, nil, nil, false
	}

	rowCount := binary.LittleEndian.Uint64(blob[8:])
	body := blob[HeaderSize:]
	n := int(rowCount)

	ts = make([]int64, n)
	px = make([]float64, n)
	qty = make([]float64, n)
	sign = make([]float64, n)
	nTrades = make([]int, n)

	invPx := 1.0 / PxScale
	invQt := 1.0 / QtScale

	rowOff := 0
	for i := 0; i < n; i++ {
		pxRaw := binary.LittleEndian.Uint64(body[rowOff+8:])
		qtyRaw := binary.LittleEndian.Uint64(body[rowOff+16:])
		cntRaw := binary.LittleEndian.Uint32(body[rowOff+32:])
		flags := binary.LittleEndian.Uint16(body[rowOff+36:])
		tsRaw := binary.LittleEndian.Uint64(body[rowOff+38:])
		rowOff += RowSize

		px[i] = float64(pxRaw) * invPx
		qty[i] = float64(qtyRaw) * invQt
		nTrades[i] = int(cntRaw)
		ts[i] = int64(tsRaw)

		// flags bit 0: 0 => taker buy, 1 => taker sell (from fastZipToAgg3)
		if (flags & 1) == 0 {
			sign[i] = 1.0
		} else {
			sign[i] = -1.0
		}
	}

	return ts, px, qty, sign, nTrades, true
}

func findBlobOFI(idxPath string, day int) (uint64, uint64) {
	f, err := os.Open(idxPath)
	if err != nil {
		return 0, 0
	}
	defer f.Close()

	hdr := make([]byte, 16)
	if _, err := io.ReadFull(f, hdr); err != nil {
		return 0, 0
	}
	if string(hdr[:4]) != IdxMagic {
		return 0, 0
	}
	count := binary.LittleEndian.Uint64(hdr[8:])

	row := make([]byte, 26)
	for i := uint64(0); i < count; i++ {
		if _, err := io.ReadFull(f, row); err != nil {
			return 0, 0
		}
		if int(binary.LittleEndian.Uint16(row[0:])) == day {
			return binary.LittleEndian.Uint64(row[2:]), binary.LittleEndian.Uint64(row[10:])
		}
	}
	return 0, 0
}

// --- Lag / Horizon Helpers --------------------------------------------------

func buildEntryIndex(ts []int64, lagMS int64) []int {
	n := len(ts)
	idx := make([]int, n)
	j := 0
	for i := 0; i < n; i++ {
		target := ts[i] + lagMS
		if j < i {
			j = i
		}
		for j < n && ts[j] < target {
			j++
		}
		if j >= n {
			idx[i] = -1
		} else {
			idx[i] = j
		}
	}
	return idx
}

type horizonData struct {
	ret   []float64
	valid []bool
}

func buildHorizonData(px []float64, entryIdx []int, horizon, warmup int) horizonData {
	n := len(px)
	ret := make([]float64, n)
	valid := make([]bool, n)

	for i := warmup; i < n; i++ {
		e := entryIdx[i]
		if e < 0 || e+horizon >= n {
			continue
		}
		pEntry := px[e]
		pExit := px[e+horizon]
		if pEntry <= 0 {
			continue
		}
		ret[i] = (pExit - pEntry) / pEntry
		valid[i] = true
	}

	return horizonData{
		ret:   ret,
		valid: valid,
	}
}

// computeSignalSeries runs one OFI variant over all ticks and returns the
// signal series (length n).
func computeSignalSeries(cfg *OFIVariant, px, qty []float64, nTrades []int, sign []float64) []float64 {
	n := len(px)
	out := make([]float64, n)
	state := newOFIState(cfg)
	for i := 0; i < n; i++ {
		out[i] = state.Update(px[i], qty[i], nTrades[i], sign[i])
	}
	return out
}

// medianDeltaMs computes median inter-trade time in milliseconds.
func medianDeltaMs(ts []int64) float64 {
	n := len(ts)
	if n < 2 {
		return 0
	}
	delta := make([]int64, n-1)
	for i := 1; i < n; i++ {
		dt := ts[i] - ts[i-1]
		if dt <= 0 {
			dt = 1
		}
		delta[i-1] = dt
	}
	sort.Slice(delta, func(i, j int) bool { return delta[i] < delta[j] })
	mid := (n - 1) / 2
	if (n-1)%2 == 1 {
		return float64(delta[mid]+delta[mid+1]) * 0.5
	}
	return float64(delta[mid])
}

// --- Core Day Processing ----------------------------------------------------

func processOFIDay(y, m, d int, horizons []int) []AlphaMetrics {
	ts, px, qty, sign, nTrades, ok := loadDayTicks(y, m, d)
	if !ok {
		return nil
	}
	n := len(px)
	if n == 0 {
		return nil
	}

	// require enough data after warmup and max horizon
	maxH := 0
	for _, h := range horizons {
		if h > maxH {
			maxH = h
		}
	}
	if n < OFIWarmupTrades+maxH+10 {
		return nil
	}

	entryIdx := buildEntryIndex(ts, int64(OFIExecutionLagMS))

	// precompute horizon returns
	hMap := make(map[int]horizonData, len(horizons))
	for _, h := range horizons {
		hMap[h] = buildHorizonData(px, entryIdx, h, OFIWarmupTrades)
	}

	medianMs := medianDeltaMs(ts)
	dateLabel := fmt.Sprintf("%04d-%02d-%02d", y, m, d)
	var results []AlphaMetrics

	for vIdx := range ofiVariants {
		variant := &ofiVariants[vIdx]

		sigSeries := computeSignalSeries(variant, px, qty, nTrades, sign)
		if len(sigSeries) == 0 {
			continue
		}

		if len(sigSeries) <= OFIWarmupTrades {
			continue
		}
		sigForMetrics := sigSeries[OFIWarmupTrades:]

		am := AlphaMetrics{
			Label:   fmt.Sprintf("%s|%s|%s", Symbol, variant.ID, dateLabel),
			NBars:   len(sigForMetrics),
			Signal:  ComputeSignalMetrics(sigForMetrics),
			Horizon: make(map[string]HorizonMetrics),
		}

		for _, h := range horizons {
			hd := hMap[h]
			var subSig, subRet []float64

			for i := OFIWarmupTrades; i < n; i++ {
				if !hd.valid[i] {
					continue
				}
				subSig = append(subSig, sigSeries[i])
				subRet = append(subRet, hd.ret[i])
			}

			if len(subSig) == 0 {
				continue
			}

			hm := ComputeHorizonMetrics(subSig, subRet)
			if hm.AlphaHalfLifeBars > 0 && medianMs > 0 {
				hm.AlphaHalfLifeMs = hm.AlphaHalfLifeBars * medianMs
			}
			am.Horizon[fmt.Sprintf("%d", h)] = hm
		}

		// Only keep if we have at least one horizon with metrics
		if len(am.Horizon) > 0 {
			results = append(results, am)
		}
	}

	return results
}

// --- Public Entry Point -----------------------------------------------------

func runOFI() {
	start := time.Now()
	root := filepath.Join(BaseDir, Symbol)

	fmt.Printf("--- OFI LAB | Symbol: %s | Lag: %dms | Horizons: %v ---\n",
		Symbol, OFIExecutionLagMS, OFIHorizons)

	years, err := os.ReadDir(root)
	if err != nil {
		fmt.Printf("[ofi] cannot read root %s: %v\n", root, err)
		return
	}

	var tasks []ofiTask
	for _, yDir := range years {
		if !yDir.IsDir() {
			continue
		}
		y, err := strconv.Atoi(yDir.Name())
		if err != nil {
			continue
		}
		months, err := os.ReadDir(filepath.Join(root, yDir.Name()))
		if err != nil {
			continue
		}
		for _, mDir := range months {
			if !mDir.IsDir() {
				continue
			}
			m, err := strconv.Atoi(mDir.Name())
			if err != nil {
				continue
			}
			idxPath := filepath.Join(root, yDir.Name(), mDir.Name(), "index.quantdev")
			if _, err := os.Stat(idxPath); err != nil {
				continue
			}
			for d := 1; d <= 31; d++ {
				tasks = append(tasks, ofiTask{Y: y, M: m, D: d})
			}
		}
	}

	fmt.Printf("[ofi] Scanning %d potential (year,month,day) slices using %d threads...\n",
		len(tasks), CPUThreads)

	jobs := make(chan ofiTask, len(tasks))
	results := make(chan []AlphaMetrics, len(tasks))
	var wg sync.WaitGroup

	for i := 0; i < CPUThreads; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			for t := range jobs {
				ms := processOFIDay(t.Y, t.M, t.D, OFIHorizons)
				if len(ms) > 0 {
					results <- ms
				}
			}
		}()
	}

	for _, t := range tasks {
		jobs <- t
	}
	close(jobs)
	wg.Wait()
	close(results)

	var all []AlphaMetrics
	for ms := range results {
		all = append(all, ms...)
	}

	if len(all) == 0 {
		fmt.Println("[ofi] No metrics produced.")
		return
	}

	// Deterministic ordering by label
	sort.Slice(all, func(i, j int) bool { return all[i].Label < all[j].Label })

	outPath := filepath.Join(BaseDir, "reports", fmt.Sprintf("ofi_%s.json", Symbol))
	if err := SaveAlphaMetrics(outPath, all); err != nil {
		fmt.Printf("[ofi] error saving metrics: %v\n", err)
	} else {
		fmt.Println("[ofi] metrics saved:", outPath)
	}

	fmt.Printf("[ofi] total records: %d | elapsed: %s\n", len(all), time.Since(start))
}
```

// --- End File: ofi.go ---

// --- File: sanity.go ---

```go
package main

import (
	"bytes"
	"compress/zlib"
	"crypto/sha256"
	"encoding/binary"
	"fmt"
	"io"
	"os"
	"path/filepath"
	"sync"
)

// runSanity verifies that all month-level data/index files are structurally sound
// and that checksums and basic header/length invariants hold.
func runSanity() {
	root := filepath.Join(BaseDir, Symbol)
	dirs, err := os.ReadDir(root)
	if err != nil {
		fmt.Printf("SANITY: cannot read root %s: %v\n", root, err)
		return
	}

	var tasks []string
	for _, y := range dirs {
		if !y.IsDir() {
			continue
		}
		months, err := os.ReadDir(filepath.Join(root, y.Name()))
		if err != nil {
			fmt.Printf("SANITY: cannot read year %s: %v\n", y.Name(), err)
			continue
		}
		for _, m := range months {
			if m.IsDir() {
				tasks = append(tasks, filepath.Join(root, y.Name(), m.Name()))
			}
		}
	}

	fmt.Printf("SANITY CHECK: %s (%d months)\n", Symbol, len(tasks))

	var wg sync.WaitGroup
	jobs := make(chan string, len(tasks))

	for i := 0; i < CPUThreads; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			for path := range jobs {
				validateMonth(path)
			}
		}()
	}

	for _, t := range tasks {
		jobs <- t
	}
	close(jobs)
	wg.Wait()
}

func validateMonth(dir string) {
	idxPath := filepath.Join(dir, "index.quantdev")
	dataPath := filepath.Join(dir, "data.quantdev")

	fIdx, err := os.Open(idxPath)
	if err != nil {
		fmt.Printf("FAIL: %s (No Index: %v)\n", dir, err)
		return
	}
	defer fIdx.Close()

	fData, err := os.Open(dataPath)
	if err != nil {
		fmt.Printf("FAIL: %s (No Data: %v)\n", dir, err)
		return
	}
	defer fData.Close()

	hdr := make([]byte, 16)
	if _, err := io.ReadFull(fIdx, hdr); err != nil {
		fmt.Printf("FAIL: %s (Index header read error: %v)\n", dir, err)
		return
	}

	if string(hdr[:4]) != IdxMagic {
		fmt.Printf("FAIL: %s (Bad index magic)\n", dir)
		return
	}

	count := binary.LittleEndian.Uint64(hdr[8:])
	row := make([]byte, 26)
	issues := 0

	for i := uint64(0); i < count; i++ {
		if _, err := io.ReadFull(fIdx, row); err != nil {
			fmt.Printf("FAIL: %s (Index row read error at %d: %v)\n", dir, i, err)
			issues++
			break
		}

		offset := int64(binary.LittleEndian.Uint64(row[2:]))
		length := int(binary.LittleEndian.Uint64(row[10:]))
		expSum := binary.LittleEndian.Uint64(row[18:])

		if length <= 0 {
			issues++
			continue
		}

		// Read compressed blob
		compData := make([]byte, length)
		if _, err := fData.Seek(offset, io.SeekStart); err != nil {
			issues++
			continue
		}
		if _, err := io.ReadFull(fData, compData); err != nil {
			issues++
			continue
		}

		r, err := zlib.NewReader(bytes.NewReader(compData))
		if err != nil {
			issues++
			continue
		}
		aggBlob, err := io.ReadAll(r)
		r.Close()
		if err != nil {
			issues++
			continue
		}

		// Check checksum
		s := sha256.Sum256(aggBlob)
		if binary.LittleEndian.Uint64(s[:8]) != expSum {
			issues++
			continue
		}

		// Basic structural checks
		if len(aggBlob) < HeaderSize {
			issues++
			continue
		}

		// Validate header magic and version minimally
		if string(aggBlob[:4]) != AggMagic {
			issues++
			continue
		}

		rowCount := binary.LittleEndian.Uint64(aggBlob[8:])
		expectedLen := HeaderSize + int(rowCount)*RowSize
		if expectedLen != len(aggBlob) {
			issues++
			continue
		}
	}

	if issues > 0 {
		fmt.Printf("ISSUES: %s (%d errors)\n", dir, issues)
	}
}
```

// --- End File: sanity.go ---

// --- File: sum.go ---

```go
package main

import (
	"encoding/json"
	"fmt"
	"os"
	"path/filepath"
	"sort"
	"strings"
	"text/tabwriter"
	"time"
)

const (
	// OFI report produced by runOFI()
	OFIReportPath = "data/reports/ofi_BTCUSDT.json"

	// Target horizon key inside AlphaMetrics.Horizon (as set in ofi.go)
	TargetHz = "50"
)

// Aggregation container per OFI variant
type ofiAgg struct {
	Variant   string
	Days      int
	TotalBars int64
	StartDate string
	EndDate   string

	SumIC       float64
	SumHAC      float64
	SumSharpe   float64
	SumProbSR   float64
	SumBE       float64
	SumFill     float64
	SumHit      float64
	SumHalfBars float64
	SumHalfMs   float64
	PosICDays   int
	PosHACDays  int
}

// runSum summarizes OFI metrics across all days and variants at TargetHz.
func runSum() {
	path := filepath.FromSlash(OFIReportPath)
	f, err := os.Open(path)
	if err != nil {
		fmt.Printf("Error opening OFI report: %v\n", err)
		return
	}
	defer f.Close()

	startT := time.Now()

	var entries []AlphaMetrics
	dec := json.NewDecoder(f)
	if err := dec.Decode(&entries); err != nil {
		fmt.Printf("Error decoding OFI report JSON: %v\n", err)
		return
	}
	if len(entries) == 0 {
		fmt.Println("No OFI metrics found.")
		return
	}

	// Aggregate per variant
	aggMap := make(map[string]*ofiAgg)

	for _, e := range entries {
		// label format: "SYMBOL|VARIANT|YYYY-MM-DD"
		parts := strings.Split(e.Label, "|")
		if len(parts) < 3 {
			continue
		}
		variant := parts[1]
		date := parts[2]

		h, ok := e.Horizon[TargetHz]
		if !ok {
			continue
		}

		a, ok := aggMap[variant]
		if !ok {
			a = &ofiAgg{Variant: variant}
			aggMap[variant] = a
		}

		a.Days++
		a.TotalBars += int64(e.NBars)

		if a.StartDate == "" || date < a.StartDate {
			a.StartDate = date
		}
		if a.EndDate == "" || date > a.EndDate {
			a.EndDate = date
		}

		a.SumIC += h.ICPearson
		a.SumHAC += h.HACSharpe
		a.SumSharpe += h.TheoreticalSharpe
		a.SumProbSR += h.ProbSharpeRatio
		a.SumBE += h.BreakevenBps
		a.SumFill += h.FillRate
		a.SumHit += h.DirectionalHit
		a.SumHalfBars += h.AlphaHalfLifeBars
		a.SumHalfMs += h.AlphaHalfLifeMs

		if h.ICPearson > 0 {
			a.PosICDays++
		}
		if h.HACSharpe > 0 {
			a.PosHACDays++
		}
	}

	if len(aggMap) == 0 {
		fmt.Println("No valid OFI metrics at target horizon.")
		return
	}

	// Convert to slice and rank by mean HAC Sharpe
	var aggs []*ofiAgg
	for _, a := range aggMap {
		aggs = append(aggs, a)
	}

	sort.Slice(aggs, func(i, j int) bool {
		di := float64(aggs[i].Days)
		dj := float64(aggs[j].Days)
		if di == 0 || dj == 0 {
			return aggs[i].Variant < aggs[j].Variant
		}
		return (aggs[i].SumHAC / di) > (aggs[j].SumHAC / dj)
	})

	// Output summary
	fmt.Printf("\n--- OFI VARIANT SUMMARY | %s | Horizon: %s ticks ---\n",
		filepath.Base(path), TargetHz)
	fmt.Printf("Processing Time: %s\n\n", time.Since(startT))

	w := tabwriter.NewWriter(os.Stdout, 0, 0, 2, ' ', 0)

	// Header
	fmt.Fprintf(w, "VARIANT\tDAYS\tBARS\tMEAN_IC\tHIT_RATE\tMEAN_HAC\tMEAN_SR\tBREAKEVEN_BPS\tFILL_RATE\tHALF_LIFE_MS\tPOS_IC%%\tPOS_HAC%%\tPERIOD\n")
	fmt.Fprintf(w, "------\t----\t----\t-------\t--------\t--------\t-------\t-------------\t---------\t------------\t-------\t--------\t------\n")

	for _, a := range aggs {
		if a.Days == 0 {
			continue
		}
		n := float64(a.Days)

		meanIC := a.SumIC / n
		meanHAC := a.SumHAC / n
		meanSR := a.SumSharpe / n
		meanBE := a.SumBE / n
		meanFill := (a.SumFill / n) * 100.0
		meanHit := (a.SumHit / n) * 100.0
		meanHLms := a.SumHalfMs / n

		posICPct := 100.0 * float64(a.PosICDays) / n
		posHACPct := 100.0 * float64(a.PosHACDays) / n

		fmt.Fprintf(w, "%s\t%d\t%d\t%.4f\t%.1f%%\t%.2f\t%.2f\t%.2f\t%.1f%%\t%.1f\t%.1f%%\t%.1f%%\t%s→%s\n",
			a.Variant,
			a.Days,
			a.TotalBars,
			meanIC,
			meanHit,
			meanHAC,
			meanSR,
			meanBE,
			meanFill,
			meanHLms,
			posICPct,
			posHACPct,
			a.StartDate,
			a.EndDate,
		)
	}

	w.Flush()
	fmt.Println("")
}
```

// --- End File: sum.go ---

