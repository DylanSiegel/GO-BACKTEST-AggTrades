--- File Tree Structure ---
|-- data/
|-- common.go
|-- data.go
|-- main.go
|-- metrics.go
|-- ofibuild.go
|-- ofistudy.go
|-- sanity.go
    |-- BTCUSDT/
    |-- ETHUSDT/
        |-- 2020/
        |-- 2021/
        |-- 2022/
        |-- 2023/
        |-- 2024/
        |-- 2025/
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..11]/ (11 month dirs)
        |-- 2020/
        |-- 2021/
        |-- 2022/
        |-- 2023/
        |-- 2024/
        |-- 2025/
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..11]/ (11 month dirs)

// --- File: common.go ---

```go
package main

import (
	"unique"
	"unsafe"
)

// --- Shared Configuration ---

const (
	// Ryzen 9 7900X: 12 Cores / 24 Threads.
	CPUThreads = 24
	BaseDir    = "data"

	// Binary Layout Constants
	PxScale    = 100_000_000.0
	QtScale    = 100_000_000.0
	HeaderSize = 48
	RowSize    = 48

	// Magic Headers
	AggMagic   = "AGG3"
	IdxMagic   = "QIDX"
	IdxVersion = 1
)

// Intern the symbol to keep it in L3 cache.
var SymbolHandle = unique.Make("ETHUSDT")

func Symbol() string { return SymbolHandle.Value() }

// AggRow corresponds to the logical fields stored in a 48-byte row.
type AggRow struct {
	TsMs       int64
	PriceFixed uint64
	QtyFixed   uint64
	Flags      uint16
}

// ParseAggRow - ZEN 4 OPTIMIZED
// Uses unsafe pointer arithmetic to bypass Go bounds checks.
// The caller GUARANTEES row has at least 48 bytes and matches the AGG3 layout.
func ParseAggRow(row []byte) AggRow {
	ptr := unsafe.Pointer(&row[0])
	return AggRow{
		// Offset 38: Timestamp (uint64)
		TsMs: int64(*(*uint64)(unsafe.Add(ptr, 38))),
		// Offset 8: Price (fixed-point, 1e-8)
		PriceFixed: *(*uint64)(unsafe.Add(ptr, 8)),
		// Offset 16: Quantity (fixed-point, 1e-8)
		QtyFixed: *(*uint64)(unsafe.Add(ptr, 16)),
		// Offset 36: Flags (uint16), bit 0 encodes is_buyer_maker.
		Flags: *(*uint16)(unsafe.Add(ptr, 36)),
	}
}

func TradePrice(row AggRow) float64 {
	return float64(row.PriceFixed) / PxScale
}

func TradeQty(row AggRow) float64 {
	return float64(row.QtyFixed) / QtScale
}

func TradeDollar(row AggRow) float64 {
	return TradePrice(row) * TradeQty(row)
}

// TradeSign:
//
//	Flags&1 == 1  -> is_buyer_maker == true -> seller-initiated -> -1
//	Flags&1 == 0  -> is_buyer_maker == false -> buyer-initiated  -> +1
func TradeSign(row AggRow) float64 {
	if row.Flags&1 != 0 {
		return -1.0
	}
	return 1.0
}
```

// --- End File: common.go ---

// --- File: data.go ---

```go
package main

import (
	"archive/zip"
	"bytes"
	"compress/zlib"
	"crypto/sha256"
	"encoding/binary"
	"fmt"
	"io"
	"math"
	"net/http"
	"os"
	"os/signal"
	"path/filepath"
	"strings"
	"sync"
	"sync/atomic"
	"time"
)

const (
	HostData   = "data.binance.vision"
	S3Prefix   = "data/futures/um"
	DataSet    = "aggTrades"
	FallbackDt = "2020-01-01"
)

var (
	httpClient *http.Client
	stopEvent  atomic.Bool
	dirLocks   sync.Map
)

func init() {
	tr := &http.Transport{
		MaxIdleConns:        100,
		MaxIdleConnsPerHost: 100,
		IdleConnTimeout:     90 * time.Second,
	}
	httpClient = &http.Client{
		Transport: tr,
		Timeout:   20 * time.Second,
	}
}

func runData() {
	sigChan := make(chan os.Signal, 1)
	// On Windows, os.Interrupt is the relevant signal.
	signal.Notify(sigChan, os.Interrupt)
	go func() {
		<-sigChan
		stopEvent.Store(true)
		fmt.Println("\n[warn] Stopping gracefully (finish current jobs)...")
	}()

	fmt.Printf("--- data.go (Ryzen 7900X Safe-Mode) | Symbol: %s ---\n", Symbol())

	start, err := time.Parse("2006-01-02", FallbackDt)
	if err != nil {
		fmt.Printf("[fatal] invalid FallbackDt %q: %v\n", FallbackDt, err)
		return
	}

	end := time.Now().UTC().AddDate(0, 0, -1)
	if end.Before(start) {
		fmt.Println("Nothing to do.")
		return
	}

	var days []time.Time
	for d := start; !d.After(end); d = d.AddDate(0, 0, 1) {
		days = append(days, d)
	}

	fmt.Printf("[job] Processing %d days using %d threads.\n", len(days), CPUThreads)

	jobs := make(chan time.Time, len(days))
	results := make(chan string, len(days))
	var wg sync.WaitGroup

	for i := 0; i < CPUThreads; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			for d := range jobs {
				if stopEvent.Load() {
					return
				}
				results <- processDay(d)
			}
		}()
	}

	for _, d := range days {
		jobs <- d
	}
	close(jobs)
	wg.Wait()
	close(results)

	stats := make(map[string]int)
	for r := range results {
		key := strings.SplitN(r, " ", 2)[0]
		stats[key]++
	}
	fmt.Printf("\n[done] %v\n", stats)
}

func processDay(d time.Time) string {
	y, m, day := d.Year(), int(d.Month()), d.Day()

	dirPath := filepath.Join(BaseDir, Symbol(), fmt.Sprintf("%04d", y), fmt.Sprintf("%02d", m))
	idxPath := filepath.Join(dirPath, "index.quantdev")
	dataPath := filepath.Join(dirPath, "data.quantdev")

	muAny, _ := dirLocks.LoadOrStore(dirPath, &sync.Mutex{})
	mu := muAny.(*sync.Mutex)

	mu.Lock()
	indexed := isIndexed(idxPath, day)
	mu.Unlock()

	if indexed {
		return "skip"
	}

	sym := Symbol()
	url := fmt.Sprintf("https://%s/%s/daily/%s/%s/%s-%s-%04d-%02d-%02d.zip",
		HostData, S3Prefix, DataSet, sym, sym, DataSet, y, m, day)

	zipBytes, err := download(url)
	if err != nil {
		if err == errNotFound {
			return "missing"
		}
		return "error_dl"
	}

	aggBlob, count, err := fastZipToAgg3(d, zipBytes)
	if err != nil {
		return "error_parse"
	}
	if count == 0 {
		return "empty"
	}

	var b bytes.Buffer
	w, err := zlib.NewWriterLevel(&b, zlib.BestSpeed)
	if err != nil {
		return "error_zlib"
	}
	if _, err := w.Write(aggBlob); err != nil {
		w.Close()
		return "error_zlib_write"
	}
	if err := w.Close(); err != nil {
		return "error_zlib_close"
	}
	compBlob := b.Bytes()

	sum := sha256.Sum256(aggBlob)
	cSum := binary.LittleEndian.Uint64(sum[:8])

	mu.Lock()
	defer mu.Unlock()

	if isIndexed(idxPath, day) {
		return "skip_race"
	}

	if err := os.MkdirAll(dirPath, 0o755); err != nil {
		return "error_mkdir"
	}

	fData, err := os.OpenFile(dataPath, os.O_CREATE|os.O_APPEND|os.O_WRONLY, 0o644)
	if err != nil {
		return "error_io"
	}
	stat, err := fData.Stat()
	if err != nil {
		fData.Close()
		return "error_stat"
	}
	offset := stat.Size()

	if _, err := fData.Write(compBlob); err != nil {
		fData.Close()
		return "error_write"
	}
	fData.Close()

	fIdx, err := os.OpenFile(idxPath, os.O_CREATE|os.O_RDWR, 0o644)
	if err != nil {
		return "error_idx"
	}
	defer fIdx.Close()

	idxStat, err := fIdx.Stat()
	if err != nil {
		return "error_idx_stat"
	}
	if idxStat.Size() == 0 {
		var hdr [16]byte
		copy(hdr[0:], IdxMagic)
		binary.LittleEndian.PutUint32(hdr[4:], uint32(IdxVersion))
		binary.LittleEndian.PutUint64(hdr[8:], 0)
		if _, err := fIdx.Write(hdr[:]); err != nil {
			return "error_idx_hdr"
		}
	}

	var row [26]byte
	binary.LittleEndian.PutUint16(row[0:], uint16(day))
	binary.LittleEndian.PutUint64(row[2:], uint64(offset))
	binary.LittleEndian.PutUint64(row[10:], uint64(len(compBlob)))
	binary.LittleEndian.PutUint64(row[18:], cSum)

	if _, err := fIdx.Seek(0, io.SeekEnd); err != nil {
		return "error_idx_seek"
	}
	if _, err := fIdx.Write(row[:]); err != nil {
		return "error_idx_write"
	}

	if _, err := fIdx.Seek(8, io.SeekStart); err != nil {
		return "error_idx_seek"
	}
	var currentCount uint64
	if err := binary.Read(fIdx, binary.LittleEndian, &currentCount); err != nil {
		return "error_idx_read"
	}
	if _, err := fIdx.Seek(8, io.SeekStart); err != nil {
		return "error_idx_seek"
	}
	if err := binary.Write(fIdx, binary.LittleEndian, currentCount+1); err != nil {
		return "error_idx_write"
	}

	return "ok"
}

// --- Helpers ---

var errNotFound = fmt.Errorf("404")

func download(url string) ([]byte, error) {
	for i := 0; i < 3; i++ {
		resp, err := httpClient.Get(url)
		if err == nil {
			if resp != nil {
				if resp.StatusCode == http.StatusOK {
					data, readErr := io.ReadAll(resp.Body)
					resp.Body.Close()
					return data, readErr
				}
				if resp.StatusCode == http.StatusNotFound {
					resp.Body.Close()
					return nil, errNotFound
				}
				resp.Body.Close()
			}
		}
		time.Sleep(time.Duration(i+1) * 100 * time.Millisecond)
	}
	return nil, fmt.Errorf("timeout")
}

func isIndexed(idxPath string, day int) bool {
	f, err := os.Open(idxPath)
	if err != nil {
		return false
	}
	defer f.Close()

	var hdr [16]byte
	if _, err := io.ReadFull(f, hdr[:]); err != nil {
		return false
	}
	if string(hdr[:4]) != IdxMagic {
		return false
	}
	count := binary.LittleEndian.Uint64(hdr[8:])

	var row [26]byte
	for i := uint64(0); i < count; i++ {
		if _, err := io.ReadFull(f, row[:]); err != nil {
			break
		}
		if int(binary.LittleEndian.Uint16(row[0:])) == day {
			return true
		}
	}
	return false
}

// parseBuyerMakerFlag interprets the is_buyer_maker CSV column.
// Expect values like "true"/"false" (case-insensitive).
// Returns bit 0 = 1 if is_buyer_maker == true.
func parseBuyerMakerFlag(col []byte) uint16 {
	if len(col) == 0 {
		return 0
	}
	switch col[0] {
	case 't', 'T': // "true"
		return 1
	default:
		return 0
	}
}

// --- CSV Parser ---
//
// CSV Layout (7 columns):
//
//	0: agg_trade_id      (uint64)
//	1: price             (float, fixed-point 1e-8)
//	2: quantity          (float, fixed-point 1e-8)
//	3: first_trade_id    (uint64)
//	4: last_trade_id     (uint64)
//	5: transact_time     (uint64, ms)
//	6: is_buyer_maker    ("true"/"false")
//
// Row layout (RowSize = 48):
//
//	0..7   : agg_trade_id (uint64)          [optional, not used downstream]
//	8..15  : price_fixed (uint64)
//	16..23 : qty_fixed   (uint64)
//	24..31 : first_trade_id (uint64)
//	32..35 : trade_count = last - first + 1 (uint32)
//	36..37 : flags (uint16), bit 0 = is_buyer_maker
//	38..45 : transact_time (uint64)
//	46..47 : unused / padding
func fastZipToAgg3(t time.Time, zipData []byte) ([]byte, uint64, error) {
	r, err := zip.NewReader(bytes.NewReader(zipData), int64(len(zipData)))
	if err != nil {
		return nil, 0, err
	}

	for _, f := range r.File {
		if !strings.HasSuffix(f.Name, ".csv") {
			continue
		}
		rc, err := f.Open()
		if err != nil {
			continue
		}

		data, err := io.ReadAll(rc)
		rc.Close()
		if err != nil {
			return nil, 0, err
		}

		estRows := len(data) / 50
		if estRows < 1 {
			estRows = 1
		}
		blob := make([]byte, 0, estRows*RowSize)
		var rowBuf [RowSize]byte

		var (
			minTs  int64 = math.MaxInt64
			maxTs  int64 = math.MinInt64
			count  uint64
			colIdx int
			start  int
			i      int
			n      = len(data)
		)

		// Skip header line
		for i < n {
			if data[i] == '\n' {
				i++
				start = i
				break
			}
			i++
		}

		for i < n {
			b := data[i]
			switch b {
			case ',':
				colSlice := data[start:i]
				switch colIdx {
				case 0:
					// agg_trade_id (stored but not used downstream)
					binary.LittleEndian.PutUint64(rowBuf[0:], fastParseUint(colSlice))
				case 1:
					// price
					binary.LittleEndian.PutUint64(rowBuf[8:], fastParseFloatFixed(colSlice))
				case 2:
					// quantity
					binary.LittleEndian.PutUint64(rowBuf[16:], fastParseFloatFixed(colSlice))
				case 3:
					// first_trade_id
					binary.LittleEndian.PutUint64(rowBuf[24:], fastParseUint(colSlice))
				case 4:
					// last_trade_id -> trade count
					fid := binary.LittleEndian.Uint64(rowBuf[24:])
					lid := fastParseUint(colSlice)
					if lid >= fid {
						binary.LittleEndian.PutUint32(rowBuf[32:], uint32(lid-fid+1))
					} else {
						binary.LittleEndian.PutUint32(rowBuf[32:], 0)
					}
				case 5:
					// transact_time
					ts := fastParseUint(colSlice)
					binary.LittleEndian.PutUint64(rowBuf[38:], ts)
					ts64 := int64(ts)
					if ts64 < minTs {
						minTs = ts64
					}
					if ts64 > maxTs {
						maxTs = ts64
					}
				}
				colIdx++
				start = i + 1

			case '\n':
				// Last column: is_buyer_maker
				colSlice := data[start:i]
				flags := parseBuyerMakerFlag(colSlice)
				binary.LittleEndian.PutUint16(rowBuf[36:], flags)

				blob = append(blob, rowBuf[:]...)
				count++
				colIdx = 0
				start = i + 1
			}
			i++
		}

		// Handle final line without trailing newline
		if colIdx == 6 && start < n {
			colSlice := data[start:n]
			flags := parseBuyerMakerFlag(colSlice)
			binary.LittleEndian.PutUint16(rowBuf[36:], flags)

			blob = append(blob, rowBuf[:]...)
			count++
		}

		if count == 0 {
			return nil, 0, nil
		}

		var hdr [HeaderSize]byte
		copy(hdr[0:], AggMagic)
		hdr[4] = 1              // version
		hdr[5] = uint8(t.Day()) // day-of-month
		binary.LittleEndian.PutUint16(hdr[6:], uint16(zlib.BestSpeed))
		binary.LittleEndian.PutUint64(hdr[8:], count)
		binary.LittleEndian.PutUint64(hdr[16:], uint64(minTs))
		binary.LittleEndian.PutUint64(hdr[24:], uint64(maxTs))

		out := make([]byte, 0, HeaderSize+len(blob))
		out = append(out, hdr[:]...)
		out = append(out, blob...)
		return out, count, nil
	}
	return nil, 0, fmt.Errorf("no csv")
}

func fastParseUint(b []byte) uint64 {
	var n uint64
	for _, c := range b {
		n = n*10 + uint64(c-'0')
	}
	return n
}

const targetDecimals = 8

var pow10 = [...]uint64{
	1, 10, 100, 1000, 10000, 100000, 1000000, 10000000, 100000000,
}

func fastParseFloatFixed(b []byte) uint64 {
	var n uint64
	seenDot := false
	decimals := 0

	for _, c := range b {
		if c == '.' {
			seenDot = true
			continue
		}
		n = n*10 + uint64(c-'0')
		if seenDot {
			decimals++
		}
	}

	if decimals == targetDecimals {
		return n
	}

	if decimals < targetDecimals {
		diff := targetDecimals - decimals
		if diff < len(pow10) {
			return n * pow10[diff]
		}
		for i := 0; i < diff; i++ {
			n *= 10
		}
		return n
	}

	diff := decimals - targetDecimals
	if diff < len(pow10) {
		return n / pow10[diff]
	}
	for i := 0; i < diff; i++ {
		n /= 10
	}
	return n
}
```

// --- End File: data.go ---

// --- File: main.go ---

```go
package main

import (
	"fmt"
	"os"
	"runtime"
	"time"
)

func main() {
	runtime.GOMAXPROCS(CPUThreads)

	if len(os.Args) < 2 {
		printHelp()
		os.Exit(1)
	}

	start := time.Now()

	fmt.Printf("Go 1.25.4 | Env: %s/%s | Threads: %d | GOGC: %s | AMD64: %s\n",
		runtime.GOOS, runtime.GOARCH,
		runtime.GOMAXPROCS(0),
		os.Getenv("GOGC"),
		os.Getenv("GOAMD64"))

	cmd := os.Args[1]

	switch cmd {
	case "data":
		runData()
	case "build":
		runBuild()
	case "study":
		runStudy()
	case "sanity":
		runSanity()
	default:
		fmt.Printf("Unknown command: %s\n", cmd)
		printHelp()
		os.Exit(1)
	}

	fmt.Printf("\n[sys] Execution Time: %s | Mem: %s\n", time.Since(start), getMemUsage())
}

func printHelp() {
	fmt.Println("Usage: quant.exe [command]")
	fmt.Println("  data   - Download raw aggTrades")
	fmt.Println("  build  - Run Hawkes/Adaptive/EMA models -> features")
	fmt.Println("  study  - Run IS/OOS backtest on features")
	fmt.Println("  sanity - Check data integrity")
}

func getMemUsage() string {
	var m runtime.MemStats
	runtime.ReadMemStats(&m)
	return fmt.Sprintf("%d MB", m.Alloc/1024/1024)
}
```

// --- End File: main.go ---

// --- File: metrics.go ---

```go
package main

import (
	"iter"
	"math"
	"sort"
)

type MetricStats struct {
	Count        int
	ICPearson    float64
	IC_TStat     float64 // Stability of IC across days
	Sharpe       float64
	HitRate      float64
	BreakevenBps float64
	AutoCorr     float64 // Lag-1 autocorrelation of signal
}

type Moments struct {
	Count     float64
	SumSig    float64
	SumRet    float64
	SumProd   float64
	SumSqSig  float64
	SumSqRet  float64
	SumPnL    float64
	SumSqPnL  float64
	Hits      float64
	ValidHits float64
	Turnover  float64

	// New: Autocorrelation terms (lag 1)
	SumProdLag  float64 // Σ(S_t * S_{t-1})
	SumSqSigLag float64 // Σ(S_{t-1}^2)
}

func (m *Moments) Add(m2 Moments) {
	m.Count += m2.Count
	m.SumSig += m2.SumSig
	m.SumRet += m2.SumRet
	m.SumProd += m2.SumProd
	m.SumSqSig += m2.SumSqSig
	m.SumSqRet += m2.SumSqRet
	m.SumPnL += m2.SumPnL
	m.SumSqPnL += m2.SumSqPnL
	m.Hits += m2.Hits
	m.ValidHits += m2.ValidHits
	m.Turnover += m2.Turnover
	m.SumProdLag += m2.SumProdLag
	m.SumSqSigLag += m2.SumSqSigLag
}

// CalcMomentsStream consumes a zero-alloc iterator and now also
// accumulates lag-1 autocorrelation terms.
func CalcMomentsStream(seq iter.Seq2[float64, float64]) Moments {
	var m Moments
	first := true
	prevSig := 0.0

	for s, r := range seq {
		m.Count++

		m.SumSig += s
		m.SumRet += r
		m.SumSqSig += s * s
		m.SumSqRet += r * r
		m.SumProd += s * r

		pnl := s * r
		m.SumPnL += pnl
		m.SumSqPnL += pnl * pnl

		if s != 0 && r != 0 {
			m.ValidHits++
			if (s > 0 && r > 0) || (s < 0 && r < 0) {
				m.Hits++
			}
		}

		if !first {
			d := s - prevSig
			if d < 0 {
				d = -d
			}
			m.Turnover += d

			// Lag-1 autocorr assuming mean ~ 0 (so we avoid a 2-pass mean correction).
			m.SumProdLag += s * prevSig
			m.SumSqSigLag += prevSig * prevSig
		} else {
			first = false
		}
		prevSig = s
	}

	return m
}

// FinalizeMetrics now takes:
//   - aggregated Moments across all days
//   - dailyICs: slice of per-day ICs for this feature/horizon
func FinalizeMetrics(m Moments, dailyICs []float64) MetricStats {
	if m.Count <= 1 {
		return MetricStats{Count: int(m.Count)}
	}

	ms := MetricStats{Count: int(m.Count)}

	// 1. Pearson IC on the full sample
	num := m.Count*m.SumProd - m.SumSig*m.SumRet
	denX := m.Count*m.SumSqSig - m.SumSig*m.SumSig
	denY := m.Count*m.SumSqRet - m.SumRet*m.SumRet
	if denX > 0 && denY > 0 {
		ms.ICPearson = num / math.Sqrt(denX*denY)
	}

	// 2. Sharpe on PnL (per-bar)
	meanPnL := m.SumPnL / m.Count
	varPnL := (m.SumSqPnL / m.Count) - (meanPnL * meanPnL)
	if varPnL > 0 {
		ms.Sharpe = meanPnL / math.Sqrt(varPnL)
	}

	// 3. Hit Rate
	if m.ValidHits > 0 {
		ms.HitRate = m.Hits / m.ValidHits
	}

	// 4. Breakeven bps per unit turnover
	if m.Turnover > 0 {
		ms.BreakevenBps = (m.SumPnL / m.Turnover) * 10000.0
	}

	// 5. Lag-1 Autocorrelation
	// Corr ≈ SumProdLag / sqrt(SumSqSig * SumSqSigLag)
	if m.SumSqSig > 0 && m.SumSqSigLag > 0 {
		ms.AutoCorr = m.SumProdLag / math.Sqrt(m.SumSqSig*m.SumSqSigLag)
	}

	// 6. IC Stability (t-stat over daily ICs)
	if len(dailyICs) > 1 {
		var sum, sumSq float64
		for _, v := range dailyICs {
			sum += v
			sumSq += v * v
		}
		mean := sum / float64(len(dailyICs))
		variance := (sumSq / float64(len(dailyICs))) - (mean * mean)
		if variance > 0 {
			stdDev := math.Sqrt(variance)
			ms.IC_TStat = mean / (stdDev / math.Sqrt(float64(len(dailyICs))))
		}
	}

	return ms
}

// --- Quantile Math (Monotonicity) ---

type BucketResult struct {
	ID        int // 1..numBuckets
	AvgSig    float64
	AvgRetBps float64
	Count     int
}

// ComputeQuantiles: signal monotonicity / shape check.
// Sorts all (sig, ret) pairs into numBuckets buckets by signal value.
func ComputeQuantiles(sigs, rets []float64, numBuckets int) []BucketResult {
	n := len(sigs)
	if n == 0 || numBuckets <= 0 {
		return nil
	}

	type pair struct {
		s, r float64
	}
	pairs := make([]pair, n)
	for i := 0; i < n; i++ {
		pairs[i] = pair{s: sigs[i], r: rets[i]}
	}

	sort.Slice(pairs, func(i, j int) bool {
		return pairs[i].s < pairs[j].s
	})

	results := make([]BucketResult, numBuckets)
	bucketSize := n / numBuckets
	if bucketSize == 0 {
		bucketSize = 1
	}

	for b := 0; b < numBuckets; b++ {
		start := b * bucketSize
		end := start + bucketSize
		if b == numBuckets-1 || end > n {
			end = n
		}
		if start >= n {
			break
		}

		var sumS, sumR float64
		count := 0
		for i := start; i < end; i++ {
			sumS += pairs[i].s
			sumR += pairs[i].r
			count++
		}
		if count == 0 {
			continue
		}

		results[b] = BucketResult{
			ID:        b + 1,
			AvgSig:    sumS / float64(count),
			AvgRetBps: (sumR / float64(count)) * 10000.0,
			Count:     count,
		}
	}

	return results
}
```

// --- End File: metrics.go ---

// --- File: ofibuild.go ---

```go
package main

import (
	"bytes"
	"compress/zlib"
	"encoding/binary"
	"fmt"
	"io"
	"math"
	"os"
	"path/filepath"
	"strconv"
	"strings"
	"sync"
	"time"
)

const BuildMaxRows = 10_000_000

// --- Configuration ---

type FiveDimConfig struct {
	RingSize     int
	Lfast, Lslow float64
	VarAlphaB    float64 // EWMA alpha for B variance
	AlphaAbs     float64 // EWMA alpha for Absorption Baseline
	VarAlphaS    float64 // EWMA alpha for Surplus variance
	AlphaE       float64 // EWMA alpha for Elasticity reference
}

type VariantDef struct {
	ID  string
	Cfg FiveDimConfig
}

// Optimized Task: Carries the exact file location to avoid redundant lookups.
type ofiTask struct {
	Y, M, D        int
	Offset, Length int64
}

func runBuild() {
	start := time.Now()

	// Adaptive Base Config
	// Lfast=2.0 (info-time), Lslow=300 (info-time) ~ roughly 2s and 5m in volume-time terms
	baseCfg := FiveDimConfig{
		RingSize:  20_000,
		Lfast:     2.0,
		Lslow:     300.0,
		VarAlphaB: 0.001, // slow variance decay
		AlphaAbs:  0.01,  // absorption baseline adaptation
		VarAlphaS: 0.005,
		AlphaE:    0.001,
	}

	variants := []VariantDef{
		{ID: "5D_Adaptive_Base", Cfg: baseCfg},
		{
			ID: "5D_Adaptive_Fast",
			Cfg: func() FiveDimConfig {
				c := baseCfg
				c.Lfast = 0.5
				c.Lslow = 60.0
				return c
			}(),
		},
	}

	symbols := discoverSymbols()
	fmt.Printf("--- FEATURE BUILDER (5D Adaptive) | Found Symbols: %v ---\n", symbols)

	for _, sym := range symbols {
		buildForSymbol(sym, variants)
	}

	fmt.Printf("[build] ALL SYMBOLS COMPLETE in %s\n", time.Since(start))
}

func discoverSymbols() []string {
	var syms []string
	entries, err := os.ReadDir(BaseDir)
	if err != nil {
		return syms
	}
	for _, e := range entries {
		if !e.IsDir() {
			continue
		}
		name := e.Name()
		if name == "features" || name == "common" || strings.HasPrefix(name, ".") {
			continue
		}
		syms = append(syms, name)
	}
	return syms
}

func buildForSymbol(sym string, variants []VariantDef) {
	fmt.Printf("\n>>> Building for %s <<<\n", sym)
	featRoot := filepath.Join(BaseDir, "features", sym)

	tasks := discoverTasks(sym)
	if len(tasks) == 0 {
		fmt.Printf("[warn] No data found for %s\n", sym)
		return
	}

	for _, v := range variants {
		buildVariant(sym, v, tasks, featRoot)
	}
}

func buildVariant(sym string, v VariantDef, tasks []ofiTask, featRoot string) {
	vStart := time.Now()
	outDir := filepath.Join(featRoot, v.ID)
	if err := os.MkdirAll(outDir, 0o755); err != nil {
		fmt.Printf("[err] mkdir %s: %v\n", outDir, err)
		return
	}

	jobs := make(chan ofiTask, len(tasks))
	var wg sync.WaitGroup

	workers := CPUThreads

	for i := 0; i < workers; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			// Buffer: 10M rows * 5 features * 8 bytes = 400MB max per day
			// We reuse this buffer across days for this worker.
			binBuf := make([]byte, 0, BuildMaxRows*5*8)
			for t := range jobs {
				processBuildDay(sym, t, outDir, v.Cfg, &binBuf)
			}
		}()
	}

	for _, t := range tasks {
		jobs <- t
	}
	close(jobs)
	wg.Wait()

	fmt.Printf("[build] %s / %s done in %s\n", sym, v.ID, time.Since(vStart))
}

func processBuildDay(
	sym string,
	t ofiTask,
	outDir string,
	cfg FiveDimConfig,
	binBuf *[]byte,
) {
	dateStr := fmt.Sprintf("%04d%02d%02d", t.Y, t.M, t.D)
	outPath := filepath.Join(outDir, dateStr+".bin")

	if _, err := os.Stat(outPath); err == nil {
		return
	}

	rawBytes, rowCount, ok := loadRawBlob(sym, t)
	if !ok || rowCount == 0 {
		return
	}

	n := int(rowCount)
	// Output: 5 float64s per row
	reqSize := n * 5 * 8
	if cap(*binBuf) < reqSize {
		*binBuf = make([]byte, reqSize)
	}
	*binBuf = (*binBuf)[:reqSize]

	// Init Engine
	core := NewCore(cfg.RingSize, cfg.Lfast, cfg.Lslow, cfg.VarAlphaB, cfg.AlphaAbs, cfg.VarAlphaS, cfg.AlphaE)
	engine := NewEngine(core)

	// Process
	for i := 0; i < n; i++ {
		off := i * RowSize
		row := ParseAggRow(rawBytes[off : off+RowSize])

		tr := Trade{
			Side:  TradeSign(row), // +1 or -1
			Qty:   TradeQty(row),
			Price: TradePrice(row),
			Ts:    row.TsMs,
		}

		feats := engine.Update(tr)

		baseOff := i * 40 // 5 * 8
		binary.LittleEndian.PutUint64((*binBuf)[baseOff+0:], math.Float64bits(feats[0]))
		binary.LittleEndian.PutUint64((*binBuf)[baseOff+8:], math.Float64bits(feats[1]))
		binary.LittleEndian.PutUint64((*binBuf)[baseOff+16:], math.Float64bits(feats[2]))
		binary.LittleEndian.PutUint64((*binBuf)[baseOff+24:], math.Float64bits(feats[3]))
		binary.LittleEndian.PutUint64((*binBuf)[baseOff+32:], math.Float64bits(feats[4]))
	}

	if err := os.WriteFile(outPath, *binBuf, 0o644); err != nil {
		fmt.Printf("[err] write %s: %v\n", outPath, err)
	}
}

// --- 5D Engine Core ---

type Trade struct {
	Side  float64
	Qty   float64
	Price float64
	Ts    int64
}

type WindowSnap struct {
	Start      int
	Count      int
	B          float64
	PriceFirst float64
	PriceLast  float64
}

type SnapPair struct {
	Fast WindowSnap
	Slow WindowSnap
}

type Core struct {
	N      int
	idx    int64
	filled bool

	u     []float64
	side  []float64
	qty   []float64
	price []float64
	ts    []int64
	info  []float64 // cumulative info-time

	Lfast float64
	Lslow float64

	// EW Vars
	EWVarBf   float64
	EWVarBs   float64
	VarAlphaB float64

	// SFA
	AbsBaseline float64
	AlphaAbs    float64
	EWVarS      float64
	VarAlphaS   float64

	// Elasticity
	ERef   float64
	AlphaE float64
}

func NewCore(N int, Lfast, Lslow, varAlphaB, alphaAbs, varAlphaS, alphaE float64) *Core {
	return &Core{
		N:         N,
		u:         make([]float64, N),
		side:      make([]float64, N),
		qty:       make([]float64, N),
		price:     make([]float64, N),
		ts:        make([]int64, N),
		info:      make([]float64, N),
		Lfast:     Lfast,
		Lslow:     Lslow,
		VarAlphaB: varAlphaB,
		AlphaAbs:  alphaAbs,
		VarAlphaS: varAlphaS,
		AlphaE:    alphaE,
	}
}

func (c *Core) Update(tr Trade) SnapPair {
	slot := int(c.idx % int64(c.N))

	// u_i = s_i * sqrt(V_i)
	u := tr.Side * math.Sqrt(tr.Qty)

	var prevInfo float64
	if c.idx > 0 {
		prevInfo = c.info[(c.idx-1)%int64(c.N)]
	}
	info := prevInfo + math.Abs(u)

	c.u[slot] = u
	c.side[slot] = tr.Side
	c.qty[slot] = tr.Qty
	c.price[slot] = tr.Price
	c.ts[slot] = tr.Ts
	c.info[slot] = info

	c.idx++
	if c.idx >= int64(c.N) {
		c.filled = true
	}

	return c.buildSnaps()
}

func (c *Core) buildSnaps() SnapPair {
	if c.idx == 0 {
		return SnapPair{}
	}
	lastPos := int((c.idx - 1) % int64(c.N))
	infoNow := c.info[lastPos]

	var Bf, Bs float64
	var countF, countS int
	startF, startS := lastPos, lastPos

	limit := c.N
	if int64(limit) > c.idx {
		limit = int(c.idx)
	}

	for k := 0; k < limit; k++ {
		i := (lastPos - k + c.N) % c.N

		// infoNow - info[i] = sum_{j=i+1..last} |u_j|
		dist := infoNow - c.info[i]
		if dist > c.Lslow {
			break
		}

		Bs += c.u[i]
		countS++
		startS = i

		if dist <= c.Lfast {
			Bf += c.u[i]
			countF++
			startF = i
		}
	}

	var fast, slow WindowSnap
	if countF > 0 {
		fast = WindowSnap{
			Start:      startF,
			Count:      countF,
			B:          Bf,
			PriceFirst: c.price[startF],
			PriceLast:  c.price[lastPos],
		}
	}
	if countS > 0 {
		slow = WindowSnap{
			Start:      startS,
			Count:      countS,
			B:          Bs,
			PriceFirst: c.price[startS],
			PriceLast:  c.price[lastPos],
		}
	}

	if fast.Count > 0 {
		diff2 := fast.B * fast.B
		if c.EWVarBf == 0 {
			c.EWVarBf = diff2
		} else {
			c.EWVarBf = (1-c.VarAlphaB)*c.EWVarBf + c.VarAlphaB*diff2
		}
	}
	if slow.Count > 0 {
		diff2 := slow.B * slow.B
		if c.EWVarBs == 0 {
			c.EWVarBs = diff2
		} else {
			c.EWVarBs = (1-c.VarAlphaB)*c.EWVarBs + c.VarAlphaB*diff2
		}
	}

	return SnapPair{Fast: fast, Slow: slow}
}

// --- Feature Engine ---

type FeatureVector [5]float64

type Engine struct {
	Core *Core
}

func NewEngine(core *Core) *Engine {
	return &Engine{Core: core}
}

func (e *Engine) Update(tr Trade) FeatureVector {
	snaps := e.Core.Update(tr)
	fSnap := snaps.Fast
	sSnap := snaps.Slow

	if fSnap.Count == 0 || sSnap.Count == 0 {
		return FeatureVector{}
	}

	f1 := e.f1_Zfast(fSnap)
	f2 := e.f2_SFA(fSnap)
	f3 := e.f3_Elasticity(fSnap)
	f4 := e.f4_Coherence(fSnap)
	f5 := e.f5_Align(fSnap, sSnap, f4)

	return FeatureVector{f1, f2, f3, f4, f5}
}

// f1: Normalized Imbalance (Fast)
func (e *Engine) f1_Zfast(f WindowSnap) float64 {
	scale := math.Sqrt(e.Core.EWVarBf + 1e-12)
	return math.Tanh(f.B / (scale + 1e-12))
}

// f2: SFA (Surplus Flow vs Absorption)
func (e *Engine) f2_SFA(f WindowSnap) float64 {
	c := e.Core
	B := f.B
	absB := math.Abs(B)

	if c.AbsBaseline == 0 {
		c.AbsBaseline = absB
	} else {
		c.AbsBaseline = (1-c.AlphaAbs)*c.AbsBaseline + c.AlphaAbs*absB
	}

	surplus := absB - c.AbsBaseline
	if surplus <= 0 {
		return 0
	}
	S := math.Copysign(surplus, B)

	diff2 := S * S
	if c.EWVarS == 0 {
		c.EWVarS = diff2
	} else {
		c.EWVarS = (1-c.VarAlphaS)*c.EWVarS + c.VarAlphaS*diff2
	}
	scale := math.Sqrt(c.EWVarS + 1e-12)
	return math.Tanh(S / (scale + 1e-12))
}

// f3: Elasticity (dP / Flow)
func (e *Engine) f3_Elasticity(f WindowSnap) float64 {
	c := e.Core
	dP := f.PriceLast - f.PriceFirst
	Q := f.B
	eMag := math.Abs(dP) / (math.Abs(Q) + 1e-12)

	if c.ERef == 0 {
		c.ERef = eMag
	} else {
		c.ERef = (1-c.AlphaE)*c.ERef + c.AlphaE*eMag
	}

	r := eMag / (c.ERef + 1e-12)
	return math.Tanh(r)
}

// f4: Coherence (Entropy of Sign Process)
func (e *Engine) f4_Coherence(f WindowSnap) float64 {
	c := e.Core
	N := f.Count
	if N < 3 {
		return 0
	}

	start := f.Start
	var npp, npm, nmp, nmm float64
	var cntP, cntM float64

	prevIdx := start
	prevSide := c.side[prevIdx]

	for k := 1; k < N; k++ {
		idx := (start + k) % c.N
		curSide := c.side[idx]

		if prevSide > 0 {
			cntP++
			if curSide > 0 {
				npp++
			} else {
				npm++
			}
		} else {
			cntM++
			if curSide > 0 {
				nmp++
			} else {
				nmm++
			}
		}

		prevIdx = idx
		prevSide = curSide
	}

	Ppp := npp / (npp + npm + 1e-12)
	Ppm := 1.0 - Ppp
	Pmp := nmp / (nmp + nmm + 1e-12)
	Pmm := 1.0 - Pmp

	piP := cntP / (float64(N-1) + 1e-12)
	piM := 1.0 - piP

	h := 0.0
	if Ppp > 1e-9 {
		h -= piP * Ppp * math.Log2(Ppp)
	}
	if Ppm > 1e-9 {
		h -= piP * Ppm * math.Log2(Ppm)
	}
	if Pmp > 1e-9 {
		h -= piM * Pmp * math.Log2(Pmp)
	}
	if Pmm > 1e-9 {
		h -= piM * Pmm * math.Log2(Pmm)
	}

	C := 1.0 - h
	if C < 0 {
		C = 0
	}
	if C > 1 {
		C = 1
	}
	return C
}

// f5: Alignment (Fast + Slow + Coherence)
func (e *Engine) f5_Align(f, s WindowSnap, C float64) float64 {
	c := e.Core

	scaleF := math.Sqrt(c.EWVarBf + 1e-12)
	scaleS := math.Sqrt(c.EWVarBs + 1e-12)

	Zf := f.B / (scaleF + 1e-12)
	Zs := s.B / (scaleS + 1e-12)

	align := math.Tanh(Zf) * math.Tanh(Zs)
	return C * align
}

// --- IO Helpers ---

func discoverTasks(sym string) []ofiTask {
	root := filepath.Join(BaseDir, sym)
	var tasks []ofiTask
	years, err := os.ReadDir(root)
	if err != nil {
		return tasks
	}

	for _, yDir := range years {
		if !yDir.IsDir() {
			continue
		}
		y, err := strconv.Atoi(yDir.Name())
		if err != nil || y <= 0 {
			continue
		}

		months, err := os.ReadDir(filepath.Join(root, yDir.Name()))
		if err != nil {
			continue
		}
		for _, mDir := range months {
			if !mDir.IsDir() {
				continue
			}
			m, err := strconv.Atoi(mDir.Name())
			if err != nil || m < 1 || m > 12 {
				continue
			}

			idxPath := filepath.Join(root, yDir.Name(), mDir.Name(), "index.quantdev")
			f, err := os.Open(idxPath)
			if err != nil {
				continue
			}
			var hdr [16]byte
			if _, err := io.ReadFull(f, hdr[:]); err != nil {
				f.Close()
				continue
			}
			count := binary.LittleEndian.Uint64(hdr[8:])
			var row [26]byte

			for i := uint64(0); i < count; i++ {
				if _, err := io.ReadFull(f, row[:]); err != nil {
					break
				}
				d := int(binary.LittleEndian.Uint16(row[0:]))
				offset := int64(binary.LittleEndian.Uint64(row[2:]))
				length := int64(binary.LittleEndian.Uint64(row[10:]))

				if d >= 1 && d <= 31 && length > 0 {
					tasks = append(tasks, ofiTask{
						Y: y, M: m, D: d,
						Offset: offset,
						Length: length,
					})
				}
			}
			f.Close()
		}
	}
	return tasks
}

func loadRawBlob(sym string, t ofiTask) ([]byte, uint64, bool) {
	dataPath := filepath.Join(BaseDir, sym, fmt.Sprintf("%04d", t.Y), fmt.Sprintf("%02d", t.M), "data.quantdev")

	f, err := os.Open(dataPath)
	if err != nil {
		return nil, 0, false
	}
	defer f.Close()

	if _, err := f.Seek(t.Offset, io.SeekStart); err != nil {
		return nil, 0, false
	}

	compData := make([]byte, t.Length)
	if _, err := io.ReadFull(f, compData); err != nil {
		return nil, 0, false
	}

	r, err := zlib.NewReader(bytes.NewReader(compData))
	if err != nil {
		return nil, 0, false
	}
	raw, err := io.ReadAll(r)
	r.Close()
	if err != nil {
		return nil, 0, false
	}

	if len(raw) < HeaderSize {
		return nil, 0, false
	}
	rowCount := binary.LittleEndian.Uint64(raw[8:])
	return raw[HeaderSize:], rowCount, true
}
```

// --- End File: ofibuild.go ---

// --- File: ofistudy.go ---

```go
package main

import (
	"bytes"
	"compress/zlib"
	"encoding/binary"
	"fmt"
	"io"
	"iter"
	"math"
	"os"
	"path/filepath"
	"slices"
	"sort"
	"strings"
	"sync"
	"text/tabwriter"
	"time"
)

// --- Configuration ---

const (
	OOSDateStr   = "2024-01-01"
	StudyMaxRows = 10_000_000
)

var TimeHorizonsSec = []int{10, 30, 60, 180, 300}
var oosBoundaryYMD int

func init() {
	oosBoundaryYMD = parseOOSBoundary(OOSDateStr)
}

type DayResult struct {
	YMD int
	// Metrics[VariantName][HorizonIdx]
	// If a file is 5D, we produce 5 virtual variants: "Var_f1", "Var_f2", ...
	Metrics map[string][]Moments
}

// --- Main Logic ---

func runStudy() {
	startT := time.Now()

	symbols := discoverFeatureSymbols()
	fmt.Printf("--- STUDY | Found Feature Sets: %v ---\n", symbols)

	for _, sym := range symbols {
		studySymbol(sym)
	}

	fmt.Printf("[study] ALL COMPLETE in %s\n", time.Since(startT))
}

func discoverFeatureSymbols() []string {
	var syms []string
	featDir := filepath.Join(BaseDir, "features")
	entries, err := os.ReadDir(featDir)
	if err != nil {
		return syms
	}
	for _, e := range entries {
		if e.IsDir() && !strings.HasPrefix(e.Name(), ".") {
			syms = append(syms, e.Name())
		}
	}
	return syms
}

func studySymbol(sym string) {
	fmt.Printf("\n>>> STUDY: %s <<<\n", sym)
	featRoot := filepath.Join(BaseDir, "features", sym)

	entries, err := os.ReadDir(featRoot)
	if err != nil {
		return
	}
	var variants []string
	for _, e := range entries {
		if e.IsDir() && !strings.HasPrefix(e.Name(), ".") {
			variants = append(variants, e.Name())
		}
	}
	slices.Sort(variants)

	if len(variants) == 0 {
		return
	}

	tasks := discoverStudyDays(filepath.Join(featRoot, variants[0]))
	fmt.Printf("Variants: %d | Days: %d\n", len(variants), len(tasks))

	// Accumulators: Map[FeatureKey] -> [Horizon]Moments
	isAcc := make(map[string][]Moments)
	oosAcc := make(map[string][]Moments)

	// Daily ICs for t-stat: Map[FeatureKey][HorizonIdx] -> []IC
	isDailyIC := make(map[string]map[int][]float64)
	oosDailyIC := make(map[string]map[int][]float64)

	var accMu sync.Mutex

	resultsChan := make(chan DayResult, 64)
	jobsChan := make(chan int, len(tasks))
	var wg sync.WaitGroup

	for i := 0; i < CPUThreads; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			// Buffers reused per thread
			prices := make([]float64, StudyMaxRows)
			times := make([]int64, StudyMaxRows)
			sigBuf := make([]float64, StudyMaxRows*5) // up to 5D
			fileBuf := make([]byte, StudyMaxRows*5*8)

			for idx := range jobsChan {
				res := processStudyDay(
					sym, tasks[idx], variants, featRoot,
					&prices, &times, &sigBuf, &fileBuf,
				)
				resultsChan <- res
			}
		}()
	}

	for i := range tasks {
		jobsChan <- i
	}
	close(jobsChan)

	go func() {
		wg.Wait()
		close(resultsChan)
	}()

	isDays, oosDays := 0, 0

	for res := range resultsChan {
		if len(res.Metrics) == 0 {
			continue
		}
		isOOS := res.YMD >= oosBoundaryYMD
		if isOOS {
			oosDays++
		} else {
			isDays++
		}

		accMu.Lock()
		for vName, moms := range res.Metrics {
			// Ensure accumulators exist
			if _, ok := isAcc[vName]; !ok {
				isAcc[vName] = make([]Moments, len(TimeHorizonsSec))
				oosAcc[vName] = make([]Moments, len(TimeHorizonsSec))
			}
			if _, ok := isDailyIC[vName]; !ok {
				isDailyIC[vName] = make(map[int][]float64)
				oosDailyIC[vName] = make(map[int][]float64)
			}

			target := isAcc[vName]
			icMap := isDailyIC[vName]
			if isOOS {
				target = oosAcc[vName]
				icMap = oosDailyIC[vName]
			}

			for hIdx := range TimeHorizonsSec {
				m := moms[hIdx]
				if m.Count <= 0 {
					continue
				}

				// Aggregate Moments
				target[hIdx].Add(m)

				// Per-day IC for stability T-stat:
				// IC = Corr(signal, return) using this day's Moments m
				num := m.Count*m.SumProd - m.SumSig*m.SumRet
				denX := m.Count*m.SumSqSig - m.SumSig*m.SumSig
				denY := m.Count*m.SumSqRet - m.SumRet*m.SumRet
				den := denX * denY

				ic := 0.0
				if den > 0 {
					ic = num / math.Sqrt(den)
				}

				icMap[hIdx] = append(icMap[hIdx], ic)
			}
		}
		accMu.Unlock()
	}

	// Collect feature keys
	var finalKeys []string
	for k := range isAcc {
		finalKeys = append(finalKeys, k)
	}
	sort.Strings(finalKeys)

	for hIdx, sec := range TimeHorizonsSec {
		printHorizonTable(
			sec,
			finalKeys,
			isAcc, oosAcc,
			isDailyIC, oosDailyIC,
			hIdx,
			isDays, oosDays,
		)
		fmt.Println()
	}
}

// Per-day processing: load raw agg data, load 1D/5D features, and
// compute Moments per feature/horizon.
func processStudyDay(
	sym string,
	dayInt int,
	variants []string,
	featRoot string,
	prices *[]float64,
	times *[]int64,
	sigBuf *[]float64,
	fileBuf *[]byte,
) DayResult {
	y, m, d := dayInt/10000, (dayInt%10000)/100, dayInt%100
	res := DayResult{YMD: dayInt, Metrics: make(map[string][]Moments)}

	rawBytes, rowCount, ok := loadRawDay(sym, y, m, d)
	if !ok || rowCount == 0 {
		return res
	}
	n := int(rowCount)

	if n > cap(*prices) {
		*prices = make([]float64, n+n/4)
	}
	if n > cap(*times) {
		*times = make([]int64, n+n/4)
	}

	p := (*prices)[:n]
	t := (*times)[:n]

	for i := 0; i < n; i++ {
		off := i * RowSize
		p[i] = float64(binary.LittleEndian.Uint64(rawBytes[off+8:]))
		t[i] = int64(binary.LittleEndian.Uint64(rawBytes[off+38:]))
	}

	dStr := fmt.Sprintf("%04d%02d%02d", y, m, d)

	for _, v := range variants {
		sigPath := filepath.Join(featRoot, v, dStr+".bin")

		rawSigs, byteSize, ok := fastLoadBytes(sigPath, fileBuf)
		if !ok {
			continue
		}

		if n == 0 {
			continue
		}
		if byteSize%(n*8) != 0 {
			continue
		}
		dims := byteSize / (n * 8)
		if dims < 1 || dims > 5 {
			continue
		}

		if n > cap(*sigBuf) {
			*sigBuf = make([]float64, n+n/4)
		}

		featureNames := []string{"f1_Z", "f2_SFA", "f3_Elast", "f4_Coh", "f5_Align"}

		for dim := 0; dim < dims; dim++ {
			target := (*sigBuf)[:n]

			for i := 0; i < n; i++ {
				offset := (i*dims + dim) * 8
				bits := binary.LittleEndian.Uint64(rawSigs[offset:])
				target[i] = math.Float64frombits(bits)
			}

			key := v
			if dims > 1 {
				suffix := fmt.Sprintf("_d%d", dim+1)
				if dim < len(featureNames) {
					suffix = "_" + featureNames[dim]
				}
				key = v + suffix
			}

			moms := make([]Moments, len(TimeHorizonsSec))
			for hIdx, sec := range TimeHorizonsSec {
				seq := AlignVectors(target, p, t, sec*1000)
				moms[hIdx] = CalcMomentsStream(seq)
			}
			res.Metrics[key] = moms
		}
	}
	return res
}

// AlignVectors: aligns signal with future returns at horizon hMs.
func AlignVectors(sig, prices []float64, times []int64, hMs int) iter.Seq2[float64, float64] {
	return func(yield func(float64, float64) bool) {
		n := len(sig)
		j := 0
		hVal := int64(hMs)

		for i := 0; i < n; i++ {
			s := sig[i]
			if s == 0 {
				continue
			}

			pStart := prices[i]
			if pStart <= 0 {
				continue
			}

			tTarget := times[i] + hVal
			if j < i+1 {
				j = i + 1
			}

			for j < n && times[j] < tTarget {
				j++
			}
			if j >= n {
				break
			}

			pEnd := prices[j]
			if pEnd > 0 {
				r := (pEnd - pStart) / pStart
				if !yield(s, r) {
					return
				}
			}
		}
	}
}

// Loads full file into buffer, returns slice and total size in bytes.
func fastLoadBytes(path string, fileBuf *[]byte) ([]byte, int, bool) {
	f, err := os.Open(path)
	if err != nil {
		return nil, 0, false
	}
	defer f.Close()

	fi, err := f.Stat()
	if err != nil {
		return nil, 0, false
	}
	size := int(fi.Size())
	if size == 0 {
		return nil, 0, false
	}

	if cap(*fileBuf) < size {
		*fileBuf = make([]byte, size)
	}
	buf := (*fileBuf)[:size]

	if _, err := io.ReadFull(f, buf); err != nil {
		return nil, 0, false
	}

	return buf, size, true
}

// --- Output / helpers ---

func printHorizonTable(
	sec int,
	keys []string,
	isAcc, oosAcc map[string][]Moments,
	isDailyIC, oosDailyIC map[string]map[int][]float64,
	hIdx, isDays, oosDays int,
) {
	w := tabwriter.NewWriter(os.Stdout, 0, 0, 2, ' ', 0)
	fmt.Fprintf(w, "== Horizon %ds [IS: %d | OOS: %d] ==\n", sec, isDays, oosDays)
	fmt.Fprintln(w, "FEATURE\tIS_IC\tIS_T\tOOS_IC\tOOS_T\tAC(Lag1)\tBPS/TR")

	for _, k := range keys {
		// Guard against missing IC slices
		var isICSlice, oosICSlice []float64
		if m, ok := isDailyIC[k]; ok {
			isICSlice = m[hIdx]
		}
		if m, ok := oosDailyIC[k]; ok {
			oosICSlice = m[hIdx]
		}

		is := FinalizeMetrics(isAcc[k][hIdx], isICSlice)
		oos := FinalizeMetrics(oosAcc[k][hIdx], oosICSlice)

		fmt.Fprintf(w, "%s\t%.4f\t%.2f\t%.4f\t%.2f\t%.3f\t%.2f\n",
			k,
			is.ICPearson, is.IC_TStat,
			oos.ICPearson, oos.IC_TStat,
			is.AutoCorr,
			is.BreakevenBps,
		)
	}
	w.Flush()
}

func discoverStudyDays(vDir string) []int {
	var days []int
	files, _ := os.ReadDir(vDir)
	for _, f := range files {
		if strings.HasSuffix(f.Name(), ".bin") {
			if val := fastAtoi(strings.TrimSuffix(f.Name(), ".bin")); val > 0 {
				days = append(days, val)
			}
		}
	}
	sort.Ints(days)
	return days
}

func parseOOSBoundary(d string) int {
	return fastAtoi(d[0:4])*10000 + fastAtoi(d[5:7])*100 + fastAtoi(d[8:10])
}

func fastAtoi(s string) int {
	n := 0
	for i := 0; i < len(s); i++ {
		c := s[i]
		if c >= '0' && c <= '9' {
			n = n*10 + int(c-'0')
		}
	}
	return n
}

func loadRawDay(sym string, y, m, d int) ([]byte, uint64, bool) {
	dir := filepath.Join(BaseDir, sym, fmt.Sprintf("%04d", y), fmt.Sprintf("%02d", m))
	idxPath := filepath.Join(dir, "index.quantdev")
	dataPath := filepath.Join(dir, "data.quantdev")

	offset, length := findBlobOffset(idxPath, d)
	if length == 0 {
		return nil, 0, false
	}

	f, err := os.Open(dataPath)
	if err != nil {
		return nil, 0, false
	}
	defer f.Close()

	if _, err := f.Seek(int64(offset), io.SeekStart); err != nil {
		return nil, 0, false
	}
	compData := make([]byte, length)
	if _, err := io.ReadFull(f, compData); err != nil {
		return nil, 0, false
	}
	r, err := zlib.NewReader(bytes.NewReader(compData))
	if err != nil {
		return nil, 0, false
	}
	raw, err := io.ReadAll(r)
	r.Close()
	if err != nil {
		return nil, 0, false
	}
	if len(raw) < HeaderSize {
		return nil, 0, false
	}
	rowCount := binary.LittleEndian.Uint64(raw[8:])
	return raw[HeaderSize:], rowCount, true
}

func findBlobOffset(idxPath string, day int) (uint64, uint64) {
	f, err := os.Open(idxPath)
	if err != nil {
		return 0, 0
	}
	defer f.Close()
	var hdr [16]byte
	if _, err := io.ReadFull(f, hdr[:]); err != nil {
		return 0, 0
	}
	count := binary.LittleEndian.Uint64(hdr[8:])
	var row [26]byte
	for i := uint64(0); i < count; i++ {
		if _, err := io.ReadFull(f, row[:]); err != nil {
			break
		}
		if int(binary.LittleEndian.Uint16(row[0:])) == day {
			return binary.LittleEndian.Uint64(row[2:]), binary.LittleEndian.Uint64(row[10:])
		}
	}
	return 0, 0
}
```

// --- End File: ofistudy.go ---

// --- File: sanity.go ---

```go
package main

import (
	"bytes"
	"compress/zlib"
	"crypto/sha256"
	"encoding/binary"
	"fmt"
	"io"
	"os"
	"path/filepath"
	"sync"
)

// runSanity scans all months for the currently active Symbol() and validates
// that index.quantdev and data.quantdev are consistent, including:
//   - presence of both files
//   - index header magic and row count
//   - compressed blobs readable and checksummed
//   - AGG3 header present and valid
//   - AGG3 body length matches HeaderSize + rowCount*RowSize
func runSanity() {
	root := filepath.Join(BaseDir, Symbol())
	dirs, err := os.ReadDir(root)
	if err != nil {
		fmt.Printf("SANITY: cannot read root %s: %v\n", root, err)
		return
	}

	var tasks []string
	for _, y := range dirs {
		if !y.IsDir() {
			continue
		}
		months, err := os.ReadDir(filepath.Join(root, y.Name()))
		if err != nil {
			fmt.Printf("SANITY: cannot read year %s: %v\n", y.Name(), err)
			continue
		}
		for _, m := range months {
			if !m.IsDir() {
				continue
			}
			tasks = append(tasks, filepath.Join(root, y.Name(), m.Name()))
		}
	}

	fmt.Printf("SANITY CHECK: %s (%d months)\n", Symbol(), len(tasks))
	if len(tasks) == 0 {
		return
	}

	var wg sync.WaitGroup
	jobs := make(chan string, len(tasks))

	for i := 0; i < CPUThreads; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			for path := range jobs {
				validateMonth(path)
			}
		}()
	}

	for _, t := range tasks {
		jobs <- t
	}
	close(jobs)
	wg.Wait()
}

func validateMonth(dir string) {
	idxPath := filepath.Join(dir, "index.quantdev")
	dataPath := filepath.Join(dir, "data.quantdev")

	fIdx, err := os.Open(idxPath)
	if err != nil {
		fmt.Printf("FAIL: %s (No Index: %v)\n", dir, err)
		return
	}
	defer fIdx.Close()

	fData, err := os.Open(dataPath)
	if err != nil {
		fmt.Printf("FAIL: %s (No Data: %v)\n", dir, err)
		return
	}
	defer fData.Close()

	var hdr [16]byte
	if _, err := io.ReadFull(fIdx, hdr[:]); err != nil {
		fmt.Printf("FAIL: %s (Header Read Error: %v)\n", dir, err)
		return
	}
	if string(hdr[:4]) != IdxMagic {
		fmt.Printf("FAIL: %s (Bad Index Magic)\n", dir)
		return
	}

	count := binary.LittleEndian.Uint64(hdr[8:])
	var row [26]byte

	issues := 0
	for i := uint64(0); i < count; i++ {
		if _, err := io.ReadFull(fIdx, row[:]); err != nil {
			issues++
			break
		}
		offset := int64(binary.LittleEndian.Uint64(row[2:]))
		length := int(binary.LittleEndian.Uint64(row[10:]))
		expSum := binary.LittleEndian.Uint64(row[18:])

		if length <= 0 {
			issues++
			continue
		}

		if _, err := fData.Seek(offset, io.SeekStart); err != nil {
			issues++
			continue
		}

		compData := make([]byte, length)
		if _, err := io.ReadFull(fData, compData); err != nil {
			issues++
			continue
		}

		r, err := zlib.NewReader(bytes.NewReader(compData))
		if err != nil {
			issues++
			continue
		}
		aggBlob, err := io.ReadAll(r)
		r.Close()
		if err != nil {
			issues++
			continue
		}

		// Check checksum of full AGG3 blob.
		s := sha256.Sum256(aggBlob)
		if binary.LittleEndian.Uint64(s[:8]) != expSum {
			issues++
			continue
		}

		// Validate AGG3 header.
		if len(aggBlob) < HeaderSize {
			issues++
			continue
		}
		if string(aggBlob[:4]) != AggMagic {
			issues++
			continue
		}

		// Optional but stronger check: rowCount vs length.
		rowCount := binary.LittleEndian.Uint64(aggBlob[8:16])
		expectedSize := HeaderSize + int(rowCount)*RowSize
		if expectedSize != len(aggBlob) {
			issues++
			continue
		}
	}

	if issues > 0 {
		fmt.Printf("ISSUES: %s (%d errors)\n", dir, issues)
	}
}
```

// --- End File: sanity.go ---

