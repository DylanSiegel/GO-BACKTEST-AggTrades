--- File Tree Structure ---
|-- data/
|-- common.go
|-- data.go
|-- main.go
|-- metrics.go
|-- ofibuild.go
|-- ofistudy.go
|-- sanity.go
    |-- BTCUSDT/
        |-- 2020/
        |-- 2021/
        |-- 2022/
        |-- 2023/
        |-- 2024/
        |-- 2025/
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..11]/ (11 month dirs)

// --- File: common.go ---

```go
package main

import (
	"encoding/binary"
)

// --- Shared Configuration ---
const (
	CPUThreads = 24
	Symbol     = "BTCUSDT"
	BaseDir    = "data"

	// Binary Layout
	PxScale    = 100_000_000.0
	QtScale    = 100_000_000.0
	HeaderSize = 48
	RowSize    = 48

	// Magic Headers
	AggMagic   = "AGG3"
	IdxMagic   = "QIDX"
	IdxVersion = 1 // Index file format version
)

// --- Zero-Alloc Trade Parsing ---

type AggRow struct {
	TsMs       int64
	PriceFixed uint64
	QtyFixed   uint64
	Flags      uint16
}

// ParseAggRow interprets a 48-byte row from data.quantdev without allocation.
func ParseAggRow(row []byte) AggRow {
	return AggRow{
		TsMs:       int64(binary.LittleEndian.Uint64(row[38:])),
		PriceFixed: binary.LittleEndian.Uint64(row[8:]),
		QtyFixed:   binary.LittleEndian.Uint64(row[16:]),
		Flags:      binary.LittleEndian.Uint16(row[36:]),
	}
}

func TradePrice(row AggRow) float64 {
	return float64(row.PriceFixed) / PxScale
}

func TradeQty(row AggRow) float64 {
	return float64(row.QtyFixed) / QtScale
}

func TradeDollar(row AggRow) float64 {
	return TradePrice(row) * TradeQty(row)
}

// TradeSign returns +1 for taker buy, -1 for taker sell.
func TradeSign(row AggRow) float64 {
	if row.Flags&1 != 0 {
		// buyer is maker -> taker is seller
		return -1.0
	}
	return 1.0
}
```

// --- End File: common.go ---

// --- File: data.go ---

```go
package main

import (
	"archive/zip"
	"bytes"
	"compress/zlib"
	"crypto/sha256"
	"encoding/binary"
	"fmt"
	"io"
	"math"
	"net/http"
	"os"
	"os/signal"
	"path/filepath"
	"strings"
	"sync"
	"syscall"
	"time"
)

// --- Local Constants (Specific to Data Downloader) ---
const (
	// Data Source
	HostData   = "data.binance.vision"
	S3Prefix   = "data/futures/um"
	DataSet    = "aggTrades"
	FallbackDt = "2020-01-01"
)

// --- Globals ---
var (
	httpClient *http.Client
	stopEvent  bool
	stopMu     sync.Mutex

	// Directory Locks to fix race conditions on monthly files
	dirLocks sync.Map
)

func init() {
	// High-throughput Transport (no TLS hacks)
	tr := &http.Transport{
		MaxIdleConns:        100,
		MaxIdleConnsPerHost: 100,
		IdleConnTimeout:     90 * time.Second,
	}
	httpClient = &http.Client{
		Transport: tr,
		Timeout:   20 * time.Second,
	}
}

// runData is called by main()
func runData() {
	// Graceful Shutdown
	sigChan := make(chan os.Signal, 1)
	signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)
	go func() {
		<-sigChan
		stopMu.Lock()
		stopEvent = true
		stopMu.Unlock()
		fmt.Println("\n[warn] Stopping gracefully (finish current jobs)...")
	}()

	fmt.Printf("--- data.go (Ryzen 7900X Optimized) | Symbol: %s ---\n", Symbol)

	start, err := time.Parse("2006-01-02", FallbackDt)
	if err != nil {
		fmt.Printf("[fatal] invalid FallbackDt %q: %v\n", FallbackDt, err)
		return
	}

	end := time.Now().UTC().AddDate(0, 0, -1)
	if end.Before(start) {
		fmt.Println("Nothing to do.")
		return
	}

	// Generate Job Queue
	var days []time.Time
	for d := start; !d.After(end); d = d.AddDate(0, 0, 1) {
		days = append(days, d)
	}

	fmt.Printf("[job] Processing %d days using %d threads.\n", len(days), CPUThreads)

	jobs := make(chan time.Time, len(days))
	results := make(chan string, len(days))
	var wg sync.WaitGroup

	// Workers
	for i := 0; i < CPUThreads; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			for d := range jobs {
				// Check Stop Signal
				stopMu.Lock()
				if stopEvent {
					stopMu.Unlock()
					return
				}
				stopMu.Unlock()

				results <- processDay(d)
			}
		}()
	}

	for _, d := range days {
		jobs <- d
	}
	close(jobs)
	wg.Wait()
	close(results)

	// Stats
	stats := make(map[string]int)
	for r := range results {
		key := strings.SplitN(r, " ", 2)[0]
		stats[key]++
	}
	fmt.Printf("\n[done] %v\n", stats)
}

func processDay(d time.Time) string {
	y, m, day := d.Year(), int(d.Month()), d.Day()

	// Paths
	dirPath := filepath.Join(BaseDir, Symbol, fmt.Sprintf("%04d", y), fmt.Sprintf("%02d", m))
	idxPath := filepath.Join(dirPath, "index.quantdev")
	dataPath := filepath.Join(dirPath, "data.quantdev")

	// 1. Get Directory Lock (serialize Index/Data per month)
	muAny, _ := dirLocks.LoadOrStore(dirPath, &sync.Mutex{})
	mu := muAny.(*sync.Mutex)

	// 2. Check Index (Fast Read)
	mu.Lock()
	indexed := isIndexed(idxPath, day)
	mu.Unlock()

	if indexed {
		return "skip"
	}

	// 3. Download (Concurrent / Slow IO)
	url := fmt.Sprintf("https://%s/%s/daily/%s/%s/%s-%s-%04d-%02d-%02d.zip",
		HostData, S3Prefix, DataSet, Symbol, Symbol, DataSet, y, m, day)

	zipBytes, err := download(url)
	if err != nil {
		if err == errNotFound {
			return "missing"
		}
		return "error_dl"
	}

	// 4. Fast Parse (Concurrent / High CPU)
	aggBlob, count, err := fastZipToAgg3(d, zipBytes)
	if err != nil {
		return "error_parse"
	}
	if count == 0 {
		return "empty"
	}

	// 5. Compress (Concurrent / High CPU)
	var b bytes.Buffer
	const zLevel = zlib.BestSpeed
	w, err := zlib.NewWriterLevel(&b, zLevel)
	if err != nil {
		return "error_zlib"
	}
	if _, err := w.Write(aggBlob); err != nil {
		w.Close()
		return "error_zlib_write"
	}
	if err := w.Close(); err != nil {
		return "error_zlib_close"
	}
	compBlob := b.Bytes()

	// 6. Checksum (Concurrent)
	sum := sha256.Sum256(aggBlob)
	cSum := binary.LittleEndian.Uint64(sum[:8])

	// 7. Write Data & Update Index (Serialized / Fast IO)
	mu.Lock()
	defer mu.Unlock()

	// Double check index in case another thread finished this day while we were processing
	if isIndexed(idxPath, day) {
		return "skip_race"
	}

	if err := os.MkdirAll(dirPath, 0755); err != nil {
		return "error_mkdir"
	}

	// Append Data
	fData, err := os.OpenFile(dataPath, os.O_CREATE|os.O_APPEND|os.O_WRONLY, 0644)
	if err != nil {
		return "error_io"
	}
	stat, err := fData.Stat()
	if err != nil {
		fData.Close()
		return "error_stat"
	}
	offset := stat.Size()

	if _, err := fData.Write(compBlob); err != nil {
		fData.Close()
		return "error_write"
	}
	if err := fData.Close(); err != nil {
		return "error_close_data"
	}

	// Append / Init Index
	fIdx, err := os.OpenFile(idxPath, os.O_CREATE|os.O_RDWR, 0644)
	if err != nil {
		return "error_idx"
	}
	defer fIdx.Close()

	idxStat, err := fIdx.Stat()
	if err != nil {
		return "error_idx_stat"
	}
	if idxStat.Size() == 0 {
		// Init Index Header
		hdr := make([]byte, 16)
		copy(hdr[0:], IdxMagic)
		binary.LittleEndian.PutUint32(hdr[4:], uint32(IdxVersion))
		binary.LittleEndian.PutUint64(hdr[8:], 0) // Count = 0
		if _, err := fIdx.Write(hdr); err != nil {
			return "error_idx_hdr"
		}
	}

	// Index Row: Day(2), Offset(8), Length(8), Checksum(8) = 26 bytes
	row := make([]byte, 26)
	binary.LittleEndian.PutUint16(row[0:], uint16(day))
	binary.LittleEndian.PutUint64(row[2:], uint64(offset))
	binary.LittleEndian.PutUint64(row[10:], uint64(len(compBlob)))
	binary.LittleEndian.PutUint64(row[18:], cSum)

	if _, err := fIdx.Seek(0, io.SeekEnd); err != nil {
		return "error_idx_seek"
	}
	if _, err := fIdx.Write(row); err != nil {
		return "error_idx_write"
	}

	// Increment Index Count (Atomic update under lock)
	if _, err := fIdx.Seek(8, io.SeekStart); err != nil {
		return "error_idx_seek"
	}
	var currentCount uint64
	if err := binary.Read(fIdx, binary.LittleEndian, &currentCount); err != nil {
		return "error_idx_read"
	}
	if _, err := fIdx.Seek(8, io.SeekStart); err != nil {
		return "error_idx_seek"
	}
	if err := binary.Write(fIdx, binary.LittleEndian, currentCount+1); err != nil {
		return "error_idx_write"
	}

	return "ok"
}

// --- Helpers ---

var errNotFound = fmt.Errorf("404")

func download(url string) ([]byte, error) {
	// Retry logic optimized for throughput
	for i := 0; i < 3; i++ {
		resp, err := httpClient.Get(url)
		if err == nil {
			if resp != nil {
				if resp.StatusCode == http.StatusOK {
					data, readErr := io.ReadAll(resp.Body)
					resp.Body.Close()
					return data, readErr
				}
				if resp.StatusCode == http.StatusNotFound {
					resp.Body.Close()
					return nil, errNotFound
				}
				resp.Body.Close()
			}
		}
		time.Sleep(time.Duration(i+1) * 100 * time.Millisecond)
	}
	return nil, fmt.Errorf("timeout")
}

func isIndexed(idxPath string, day int) bool {
	f, err := os.Open(idxPath)
	if err != nil {
		return false
	}
	defer f.Close()

	// Read Header
	hdr := make([]byte, 16)
	if _, err := io.ReadFull(f, hdr); err != nil {
		return false
	}
	if string(hdr[:4]) != IdxMagic {
		return false
	}
	count := binary.LittleEndian.Uint64(hdr[8:])

	// Scan Rows (max 31)
	row := make([]byte, 26)
	for i := uint64(0); i < count; i++ {
		if _, err := io.ReadFull(f, row); err != nil {
			break
		}
		if int(binary.LittleEndian.Uint16(row[0:])) == day {
			return true
		}
	}
	return false
}

// fastZipToAgg3: Zero-alloc column scanning + Binary Packing
func fastZipToAgg3(t time.Time, zipData []byte) ([]byte, uint64, error) {
	r, err := zip.NewReader(bytes.NewReader(zipData), int64(len(zipData)))
	if err != nil {
		return nil, 0, err
	}

	const zLevel = zlib.BestSpeed // informational only, used in header

	for _, f := range r.File {
		if !strings.HasSuffix(f.Name, ".csv") {
			continue
		}
		rc, err := f.Open()
		if err != nil {
			continue
		}

		// 32GB RAM allows reading full CSV.
		data, err := io.ReadAll(rc)
		rc.Close()
		if err != nil {
			return nil, 0, err
		}

		// Estimation: ~70 bytes CSV -> 48 bytes Bin
		estRows := len(data) / 50
		if estRows < 1 {
			estRows = 1
		}
		blob := make([]byte, 0, estRows*RowSize)
		rowBuf := make([]byte, RowSize)

		var (
			minTs int64 = math.MaxInt64
			maxTs int64 = math.MinInt64
			count uint64

			// Column Tracking
			// 0:id, 1:px, 2:qty, 3:fid, 4:lid, 5:ts, 6:m
			colIdx int
			start  int
			i      int
			n      = len(data)
		)

		// Skip Header Line
		for i < n {
			if data[i] == '\n' {
				i++
				start = i
				break
			}
			i++
		}

		// State Machine Loop
		for i < n {
			b := data[i]

			switch b {
			case ',':
				colSlice := data[start:i]

				switch colIdx {
				case 0: // AggID
					binary.LittleEndian.PutUint64(rowBuf[0:], fastParseUint(colSlice))
				case 1: // Price
					binary.LittleEndian.PutUint64(rowBuf[8:], fastParseFloatFixed(colSlice))
				case 2: // Qty
					binary.LittleEndian.PutUint64(rowBuf[16:], fastParseFloatFixed(colSlice))
				case 3: // FirstID
					binary.LittleEndian.PutUint64(rowBuf[24:], fastParseUint(colSlice))
				case 4: // LastID -> Count
					fid := binary.LittleEndian.Uint64(rowBuf[24:])
					lid := fastParseUint(colSlice)
					binary.LittleEndian.PutUint32(rowBuf[32:], uint32(lid-fid+1))
				case 5: // Time
					ts := fastParseUint(colSlice)
					binary.LittleEndian.PutUint64(rowBuf[38:], ts)
					if int64(ts) < minTs {
						minTs = int64(ts)
					}
					if int64(ts) > maxTs {
						maxTs = int64(ts)
					}
				}

				colIdx++
				start = i + 1

			case '\n':
				// End of row (Maker Flag)
				colSlice := data[start:i]

				// Case 6: is_buyer_maker
				flags := uint16(0)
				if len(colSlice) > 0 {
					c := colSlice[0]
					if c == 't' || c == 'T' {
						flags = 1
					}
				}
				binary.LittleEndian.PutUint16(rowBuf[36:], flags)

				// Append Row
				blob = append(blob, rowBuf...)
				count++

				colIdx = 0
				start = i + 1
			}
			i++
		}

		// Handle case where file doesn't end with \n
		if colIdx == 6 && start < n {
			colSlice := data[start:n]
			flags := uint16(0)
			if len(colSlice) > 0 && (colSlice[0] == 't' || colSlice[0] == 'T') {
				flags = 1
			}
			binary.LittleEndian.PutUint16(rowBuf[36:], flags)
			blob = append(blob, rowBuf...)
			count++
		}

		if count == 0 {
			return nil, 0, nil
		}

		// Header construction (Matches AggHeader: 48 bytes)
		hdr := make([]byte, HeaderSize)
		copy(hdr[0:], AggMagic)
		hdr[4] = 1
		hdr[5] = uint8(t.Day())
		binary.LittleEndian.PutUint16(hdr[6:], uint16(zLevel)) // zlib level (informational)
		binary.LittleEndian.PutUint64(hdr[8:], count)
		binary.LittleEndian.PutUint64(hdr[16:], uint64(minTs))
		binary.LittleEndian.PutUint64(hdr[24:], uint64(maxTs))
		// Bytes 32-47 are padding (zero initialized by make)

		return append(hdr, blob...), count, nil
	}
	return nil, 0, fmt.Errorf("no csv")
}

// --- High Performance Parsers (No Alloc) ---

func fastParseUint(b []byte) uint64 {
	var n uint64
	for _, c := range b {
		n = n*10 + uint64(c-'0')
	}
	return n
}

// Converts "123.45" -> 12345000000 (scaled 1e8)
func fastParseFloatFixed(b []byte) uint64 {
	var n uint64
	seenDot := false
	decimals := 0

	for _, c := range b {
		if c == '.' {
			seenDot = true
			continue
		}
		n = n*10 + uint64(c-'0')
		if seenDot {
			decimals++
		}
	}

	// Adjust Scale 1e8
	const target = 8
	if decimals < target {
		for i := 0; i < (target - decimals); i++ {
			n *= 10
		}
	} else if decimals > target {
		for i := 0; i < (decimals - target); i++ {
			n /= 10
		}
	}
	return n
}
```

// --- End File: data.go ---

// --- File: main.go ---

```go
package main

import (
	"fmt"
	"os"
	"runtime"
	"time"
)

func main() {
	// Hardware Optimization: Ryzen 9 7900X
	runtime.GOMAXPROCS(CPUThreads)

	if len(os.Args) < 2 {
		printHelp()
		os.Exit(1)
	}

	start := time.Now()
	cmd := os.Args[1]

	switch cmd {
	case "data":
		runData() // Download
	case "build":
		runBuild() // Run Models -> .bin
	case "study":
		runStudy() // IS/OOS Stats
	case "sanity":
		runSanity() // Integrity Check
	default:
		fmt.Printf("Unknown command: %s\n", cmd)
		printHelp()
		os.Exit(1)
	}

	fmt.Printf("\n[sys] Execution Time: %s | Mem: %s\n", time.Since(start), getMemUsage())
}

func printHelp() {
	fmt.Println("Usage: quant.exe [command]")
	fmt.Println("  data   - Download raw aggTrades")
	fmt.Println("  build  - Run Hawkes/Adaptive/EMA models -> features")
	fmt.Println("  study  - Run IS/OOS backtest on features")
	fmt.Println("  sanity - Check data integrity")
}

func getMemUsage() string {
	var m runtime.MemStats
	runtime.ReadMemStats(&m)
	return fmt.Sprintf("%d MB", m.Alloc/1024/1024)
}
```

// --- End File: main.go ---

// --- File: metrics.go ---

```go
package main

import (
	"math"
)

// MetricStats holds the metrics actually used by the study path.
type MetricStats struct {
	Count        int
	ICPearson    float64 // linear IC
	Sharpe       float64 // per-trade Sharpe
	SharpeAnnual float64 // scaled Sharpe (sqrt(N) over sample)
	HitRate      float64 // directional accuracy (non-zero signal & return)
	BreakevenBps float64 // break-even cost per unit turnover (bps)
}

// ComputeStats calculates IC, Sharpe, hit rate, and break-even cost
// for a signal vs return series. sig and ret must be aligned and of
// equal length. This function is called very frequently in the study
// pipeline and is written to be single-pass and branch-light.
func ComputeStats(sig, ret []float64) MetricStats {
	n := len(sig)
	if n < 2 {
		return MetricStats{}
	}

	ms := MetricStats{Count: n}

	var (
		sumSig, sumRet     float64
		sumSqSig, sumSqRet float64
		sumProd            float64

		// PnL aggregates
		sumPnL, sumSqPnL float64

		// Hit-rate aggregates
		hits, validHits float64

		// Turnover
		turnover float64
		prevSig  float64
	)

	for i := 0; i < n; i++ {
		s := sig[i]
		r := ret[i]

		// Moments for Pearson IC
		sumSig += s
		sumRet += r
		sumSqSig += s * s
		sumSqRet += r * r
		sumProd += s * r

		// PnL stats
		pnl := s * r
		sumPnL += pnl
		sumSqPnL += pnl * pnl

		// Hit rate (directional)
		if s != 0 && r != 0 {
			validHits++
			if (s > 0 && r > 0) || (s < 0 && r < 0) {
				hits++
			}
		}

		// Turnover (L1 change of signal)
		if i > 0 {
			turnover += math.Abs(s - prevSig)
		}
		prevSig = s
	}

	fn := float64(n)

	// 1) Pearson IC
	num := fn*sumProd - sumSig*sumRet
	denX := fn*sumSqSig - sumSig*sumSig
	denY := fn*sumSqRet - sumRet*sumRet
	if denX > 0 && denY > 0 {
		ms.ICPearson = num / math.Sqrt(denX*denY)
	}

	// 2) Sharpe per trade and "annualized" via sqrt(N) over sample.
	meanPnL := sumPnL / fn
	varPnL := (sumSqPnL / fn) - (meanPnL * meanPnL)
	if varPnL > 0 {
		stdPnL := math.Sqrt(varPnL)
		sh := meanPnL / stdPnL
		ms.Sharpe = sh
		ms.SharpeAnnual = sh * math.Sqrt(fn)
	}

	// 3) Hit rate
	if validHits > 0 {
		ms.HitRate = hits / validHits
	}

	// 4) Break-even cost in bps per unit turnover.
	if turnover > 0 {
		ms.BreakevenBps = (sumPnL / turnover) * 10000.0
	}

	return ms
}
```

// --- End File: metrics.go ---

// --- File: ofibuild.go ---

```go
package main

import (
	"bytes"
	"compress/zlib"
	"encoding/binary"
	"fmt"
	"io"
	"math"
	"os"
	"path/filepath"
	"strconv"
	"sync"
	"time"
)

// --- Build Configuration ---

// Concurrency and buffer sizing for the feature builder.
const (
	BuildThreads = 24         // use all 24 logical cores
	BuildMaxRows = 10_000_000 // initial per-thread row budget for signal buffers
)

var BuildVariants = []VariantDef{
	{
		ID:    "A_Hawkes_Core",
		Model: "Hawkes2Scale",
		Cfg: Hawkes2ScaleConfig{
			TauFast: 5, TauSlow: 60,
			MuBuy: 0.1, MuSell: 0.1,
			A_pp_fast: 0.8, A_pm_fast: 0.2, A_mp_fast: 0.2, A_mm_fast: 0.8,
			A_pp_slow: 0.4, A_pm_slow: 0.1, A_mp_slow: 0.1, A_mm_slow: 0.4,
			D0: 50_000, VolLambda: 0.999, ZScoreLambda: 0.9995, SquashScale: 1.5,
		},
	},
	{
		ID:    "B_Hawkes_Adaptive",
		Model: "HawkesAdaptive",
		Cfg: HawkesAdaptiveConfig{
			HawkesCfg: Hawkes2ScaleConfig{
				TauFast: 2, TauSlow: 300,
				MuBuy: 0.1, MuSell: 0.1,
				A_pp_fast: 1.2, A_pm_fast: 0.0, A_mp_fast: 0.0, A_mm_fast: 1.2,
				A_pp_slow: 0.3, A_pm_slow: 0.1, A_mp_slow: 0.1, A_mm_slow: 0.3,
				D0: 50_000, VolLambda: 0.999, ZScoreLambda: 0.9999, SquashScale: 2.0,
			},
			ActivityLambda: 0.99, ActMid: 15.0, ActSlope: 2.0,
		},
	},
	{
		ID:    "C_MultiEMA_PowerLaw",
		Model: "MultiEMA",
		Cfg: MultiEMAConfig{
			TauSec:       []float64{10, 60, 300, 1800},
			Weights:      []float64{0.4, 0.3, 0.2, 0.1},
			D0:           50_000,
			VolLambda:    0.999,
			ZScoreLambda: 0.9995,
			SquashScale:  1.5,
		},
	},
	{
		ID:    "D_EMA_Baseline",
		Model: "EMA",
		Cfg: EMAConfig{
			TauSec:       30,
			D0:           50_000,
			VolLambda:    0.999,
			ZScoreLambda: 0.9995,
			SquashScale:  1.5,
		},
	},
}

// --- Main Builder ---

func runBuild() {
	start := time.Now()
	featRoot := filepath.Join(BaseDir, "features", Symbol)
	fmt.Printf("--- FEATURE BUILDER | %s | %d Variants ---\n", Symbol, len(BuildVariants))
	fmt.Printf("Output: %s\n", featRoot)

	// 1. Initialize Directories for each variant
	for _, v := range BuildVariants {
		_ = os.MkdirAll(filepath.Join(featRoot, v.ID), 0755)
	}

	// 2. Discover Tasks (only days that actually exist in index.quantdev)
	tasks := discoverTasks()
	fmt.Printf("[build] Processing %d days.\n", len(tasks))

	// Worker count (bounded by CPUThreads)
	workerCount := BuildThreads
	if workerCount > CPUThreads {
		workerCount = CPUThreads
	}
	fmt.Printf("[build] Using %d threads.\n", workerCount)

	// 3. Worker Pool
	var wg sync.WaitGroup
	jobs := make(chan ofiTask, len(tasks))

	for i := 0; i < workerCount; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			// Per-thread buffer reused to reduce GC.
			// Initial capacity is BuildMaxRows, but will grow if a day exceeds that.
			binBuf := make([]byte, 0, BuildMaxRows*8)

			for t := range jobs {
				processBuildDay(t, featRoot, &binBuf)
			}
		}()
	}

	for _, t := range tasks {
		jobs <- t
	}
	close(jobs)
	wg.Wait()

	fmt.Printf("[build] Complete in %s\n", time.Since(start))
}

// processBuildDay: load raw AGG3 data ONCE for (Y,M,D), then fan out to all variants.
func processBuildDay(t ofiTask, root string, binBuf *[]byte) {
	// 1. Load Data
	rawBytes, rowCount, ok := loadRawDay(t.Y, t.M, t.D)
	if !ok || rowCount == 0 {
		return
	}
	dateStr := fmt.Sprintf("%04d%02d%02d", t.Y, t.M, t.D)

	// 2. Process Each Variant
	for _, v := range BuildVariants {
		outPath := filepath.Join(root, v.ID, dateStr+".bin")

		// Skip if exists
		if _, err := os.Stat(outPath); err == nil {
			continue
		}

		// 3. Init Model State
		model := createModel(v)
		if model == nil {
			continue
		}

		// 4. Run Update Loop
		n := int(rowCount)
		reqSize := n * 8
		if cap(*binBuf) < reqSize {
			// In the rare case a day exceeds BuildMaxRows, grow the buffer once.
			*binBuf = make([]byte, reqSize)
		}
		*binBuf = (*binBuf)[:reqSize]

		for i := 0; i < n; i++ {
			off := i * RowSize
			row := ParseAggRow(rawBytes[off : off+RowSize])

			sig := model.Update(row)

			// Store as little-endian float64
			binary.LittleEndian.PutUint64((*binBuf)[i*8:], math.Float64bits(sig))
		}

		// 5. Write Disk
		if err := os.WriteFile(outPath, *binBuf, 0644); err != nil {
			fmt.Printf("[err] write %s: %v\n", outPath, err)
		}
	}
}

// --- Data Loader ---

func loadRawDay(y, m, d int) ([]byte, uint64, bool) {
	dir := filepath.Join(BaseDir, Symbol, fmt.Sprintf("%04d", y), fmt.Sprintf("%02d", m))
	idxPath := filepath.Join(dir, "index.quantdev")
	dataPath := filepath.Join(dir, "data.quantdev")

	offset, length := findBlobOffset(idxPath, d)
	if length == 0 {
		return nil, 0, false
	}

	f, err := os.Open(dataPath)
	if err != nil {
		return nil, 0, false
	}
	defer f.Close()

	if _, err := f.Seek(int64(offset), io.SeekStart); err != nil {
		return nil, 0, false
	}

	compData := make([]byte, length)
	if _, err := io.ReadFull(f, compData); err != nil {
		return nil, 0, false
	}

	r, err := zlib.NewReader(bytes.NewReader(compData))
	if err != nil {
		return nil, 0, false
	}
	raw, err := io.ReadAll(r)
	r.Close()
	if err != nil {
		return nil, 0, false
	}

	if len(raw) < HeaderSize {
		return nil, 0, false
	}
	rowCount := binary.LittleEndian.Uint64(raw[8:])
	return raw[HeaderSize:], rowCount, true
}

func findBlobOffset(idxPath string, day int) (uint64, uint64) {
	f, err := os.Open(idxPath)
	if err != nil {
		return 0, 0
	}
	defer f.Close()

	hdr := make([]byte, 16)
	if _, err := io.ReadFull(f, hdr); err != nil {
		return 0, 0
	}
	count := binary.LittleEndian.Uint64(hdr[8:])
	row := make([]byte, 26)
	for i := uint64(0); i < count; i++ {
		if _, err := io.ReadFull(f, row); err != nil {
			break
		}
		if int(binary.LittleEndian.Uint16(row[0:])) == day {
			return binary.LittleEndian.Uint64(row[2:]), binary.LittleEndian.Uint64(row[10:])
		}
	}
	return 0, 0
}

// discoverTasks builds a list of (Y,M,D) that actually exist in the index files.
// This avoids scheduling days that don't exist and reduces wasted I/O.
func discoverTasks() []ofiTask {
	root := filepath.Join(BaseDir, Symbol)
	var tasks []ofiTask

	years, err := os.ReadDir(root)
	if err != nil {
		return tasks
	}

	for _, yDir := range years {
		if !yDir.IsDir() {
			continue
		}
		y, err := strconv.Atoi(yDir.Name())
		if err != nil || y <= 0 {
			continue
		}

		months, err := os.ReadDir(filepath.Join(root, yDir.Name()))
		if err != nil {
			continue
		}
		for _, mDir := range months {
			if !mDir.IsDir() {
				continue
			}
			m, err := strconv.Atoi(mDir.Name())
			if err != nil || m < 1 || m > 12 {
				continue
			}

			idxPath := filepath.Join(root, yDir.Name(), mDir.Name(), "index.quantdev")
			f, err := os.Open(idxPath)
			if err != nil {
				continue
			}

			hdr := make([]byte, 16)
			if _, err := io.ReadFull(f, hdr); err != nil {
				f.Close()
				continue
			}
			// Optional: verify magic
			if string(hdr[:4]) != IdxMagic {
				f.Close()
				continue
			}

			count := binary.LittleEndian.Uint64(hdr[8:])
			row := make([]byte, 26)
			for i := uint64(0); i < count; i++ {
				if _, err := io.ReadFull(f, row); err != nil {
					break
				}
				d := int(binary.LittleEndian.Uint16(row[0:]))
				if d >= 1 && d <= 31 {
					tasks = append(tasks, ofiTask{Y: y, M: m, D: d})
				}
			}
			f.Close()
		}
	}
	return tasks
}

type ofiTask struct{ Y, M, D int }

// --- Model Logic & State Definitions ---

type VariantDef struct {
	ID    string
	Model string
	Cfg   interface{}
}

type SignalModel interface {
	Update(row AggRow) float64
}

func createModel(v VariantDef) SignalModel {
	switch v.Model {
	case "Hawkes2Scale":
		return NewHawkes2ScaleState(v.Cfg.(Hawkes2ScaleConfig))
	case "HawkesAdaptive":
		return NewHawkesAdaptiveState(v.Cfg.(HawkesAdaptiveConfig))
	case "MultiEMA":
		return NewMultiEMAState(v.Cfg.(MultiEMAConfig))
	case "EMA":
		return NewEMAState(v.Cfg.(EMAConfig))
	}
	return nil
}

// --- Shared Wrappers ---

type VolEWMA struct {
	Lambda  float64
	VarEwma float64
	LastPx  float64
	HasLast bool
}

func (v *VolEWMA) Update(price float64) {
	if !v.HasLast {
		v.LastPx = price
		v.HasLast = true
		return
	}
	if price <= 0 {
		return
	}
	r := math.Log(price / v.LastPx)
	v.LastPx = price
	v.VarEwma = v.Lambda*v.VarEwma + (1-v.Lambda)*r*r
}

func (v *VolEWMA) Sigma() float64 {
	if v.VarEwma <= 0 {
		return 0
	}
	return math.Sqrt(v.VarEwma)
}

type ZScoreEWMA struct {
	Lambda float64
	Mean   float64
	Var    float64
	Warmed bool
}

func (z *ZScoreEWMA) Update(x float64) float64 {
	if !z.Warmed {
		z.Mean = x
		z.Var = 0
		z.Warmed = true
		return 0
	}
	mPrev := z.Mean
	z.Mean = z.Lambda*z.Mean + (1-z.Lambda)*x
	dx := x - mPrev
	z.Var = z.Lambda*z.Var + (1-z.Lambda)*dx*dx
	if z.Var <= 0 {
		return 0
	}
	return (x - z.Mean) / math.Sqrt(z.Var)
}

func Squash(x, scale float64) float64 {
	return math.Tanh(scale * x)
}

// --- Model A: Hawkes 2-Scale ---

type Hawkes2ScaleConfig struct {
	TauFast, TauSlow, MuBuy, MuSell            float64
	A_pp_fast, A_pm_fast, A_mp_fast, A_mm_fast float64
	A_pp_slow, A_pm_slow, A_mp_slow, A_mm_slow float64
	D0, VolLambda, ZScoreLambda, SquashScale   float64
}

type Hawkes2ScaleState struct {
	cfg                   Hawkes2ScaleConfig
	lastTsMs              int64
	eBuyFast, eSellFast   float64
	eBuySlow, eSellSlow   float64
	lambdaBuy, lambdaSell float64
	vol                   VolEWMA
	z                     ZScoreEWMA
}

func NewHawkes2ScaleState(cfg Hawkes2ScaleConfig) *Hawkes2ScaleState {
	return &Hawkes2ScaleState{
		cfg: cfg,
		vol: VolEWMA{Lambda: cfg.VolLambda},
		z:   ZScoreEWMA{Lambda: cfg.ZScoreLambda},
	}
}

func (st *Hawkes2ScaleState) Update(row AggRow) float64 {
	ts := row.TsMs
	d := TradeDollar(row)
	s := TradeSign(row)
	px := TradePrice(row)

	// dt in seconds
	var dtSec float64
	if st.lastTsMs == 0 {
		st.lastTsMs = ts
		dtSec = 0
	} else {
		dtSec = float64(ts-st.lastTsMs) / 1000.0
		if dtSec < 0 {
			dtSec = 0
		}
		st.lastTsMs = ts
	}

	// decay excitations
	if dtSec > 0 {
		df := math.Exp((-1.0 / st.cfg.TauFast) * dtSec)
		ds := math.Exp((-1.0 / st.cfg.TauSlow) * dtSec)
		st.eBuyFast *= df
		st.eSellFast *= df
		st.eBuySlow *= ds
		st.eSellSlow *= ds
	}

	// log-saturated mark
	mark := 0.0
	if d > 0 && st.cfg.D0 > 0 {
		mark = math.Log(1.0 + d/st.cfg.D0)
	}

	// add excitation
	if s > 0 {
		st.eBuyFast += mark
		st.eBuySlow += mark
	} else {
		st.eSellFast += mark
		st.eSellSlow += mark
	}

	// intensities
	buy := st.cfg.MuBuy +
		(st.cfg.A_pp_fast*st.eBuyFast + st.cfg.A_pm_fast*st.eSellFast) +
		(st.cfg.A_pp_slow*st.eBuySlow + st.cfg.A_pm_slow*st.eSellSlow)

	sell := st.cfg.MuSell +
		(st.cfg.A_mp_fast*st.eBuyFast + st.cfg.A_mm_fast*st.eSellFast) +
		(st.cfg.A_mp_slow*st.eBuySlow + st.cfg.A_mm_slow*st.eSellSlow)

	if buy < 0 {
		buy = 0
	}
	if sell < 0 {
		sell = 0
	}

	st.lambdaBuy = buy
	st.lambdaSell = sell

	imb := 0.0
	if den := buy + sell; den > 1e-12 {
		imb = (buy - sell) / den
	}

	// vol & z
	st.vol.Update(px)
	sigma := st.vol.Sigma()
	if sigma <= 0 {
		sigma = 1
	}
	return Squash(st.z.Update(imb/sigma), st.cfg.SquashScale)
}

// --- Model B: Hawkes Adaptive ---

type HawkesAdaptiveConfig struct {
	HawkesCfg                        Hawkes2ScaleConfig
	ActivityLambda, ActMid, ActSlope float64
}

type HawkesAdaptiveState struct {
	base                                         Hawkes2ScaleConfig
	lastTsMs                                     int64
	eBuyFast, eSellFast                          float64
	eBuySlow, eSellSlow                          float64
	actLambda, actEWMA, actMid, actSlope, squash float64
	vol                                          VolEWMA
	z                                            ZScoreEWMA
}

func NewHawkesAdaptiveState(cfg HawkesAdaptiveConfig) *HawkesAdaptiveState {
	return &HawkesAdaptiveState{
		base:      cfg.HawkesCfg,
		actLambda: cfg.ActivityLambda,
		actMid:    cfg.ActMid,
		actSlope:  cfg.ActSlope,
		squash:    cfg.HawkesCfg.SquashScale,
		vol:       VolEWMA{Lambda: cfg.HawkesCfg.VolLambda},
		z:         ZScoreEWMA{Lambda: cfg.HawkesCfg.ZScoreLambda},
	}
}

func (st *HawkesAdaptiveState) Update(row AggRow) float64 {
	ts := row.TsMs
	d := TradeDollar(row)
	s := TradeSign(row)
	px := TradePrice(row)

	var dtSec float64
	if st.lastTsMs == 0 {
		st.lastTsMs = ts
		dtSec = 0
	} else {
		dtSec = float64(ts-st.lastTsMs) / 1000.0
		if dtSec < 0 {
			dtSec = 0
		}
		st.lastTsMs = ts
	}

	// activity EWMA and decay
	if dtSec > 0 {
		st.actEWMA = st.actLambda*st.actEWMA + (1-st.actLambda)*(1.0/dtSec)
		df := math.Exp((-1.0 / st.base.TauFast) * dtSec)
		ds := math.Exp((-1.0 / st.base.TauSlow) * dtSec)
		st.eBuyFast *= df
		st.eSellFast *= df
		st.eBuySlow *= ds
		st.eSellSlow *= ds
	}

	// mark
	mark := 0.0
	if d > 0 && st.base.D0 > 0 {
		mark = math.Log(1.0 + d/st.base.D0)
	}

	if s > 0 {
		st.eBuyFast += mark
		st.eBuySlow += mark
	} else {
		st.eSellFast += mark
		st.eSellSlow += mark
	}

	// fast kernel intensities
	bf := st.base.MuBuy + st.base.A_pp_fast*st.eBuyFast + st.base.A_pm_fast*st.eSellFast
	sf := st.base.MuSell + st.base.A_mp_fast*st.eBuyFast + st.base.A_mm_fast*st.eSellFast

	// slow kernel intensities
	bs := st.base.MuBuy + st.base.A_pp_slow*st.eBuySlow + st.base.A_pm_slow*st.eSellSlow
	ss := st.base.MuSell + st.base.A_mp_slow*st.eBuySlow + st.base.A_mm_slow*st.eSellSlow

	// activity-based slow weight (higher activity -> lower slow weight)
	wSlow := 0.5
	if st.actEWMA > 0 {
		x := (math.Log(st.actEWMA+1e-9) - math.Log(st.actMid+1e-9)) * st.actSlope
		wSlow = 1.0 / (1.0 + math.Exp(x))
	}
	if wSlow < 0 {
		wSlow = 0
	}
	if wSlow > 1 {
		wSlow = 1
	}
	wFast := 1.0 - wSlow

	buy := wFast*bf + wSlow*bs
	sell := wFast*sf + wSlow*ss
	if buy < 0 {
		buy = 0
	}
	if sell < 0 {
		sell = 0
	}

	imb := 0.0
	if den := buy + sell; den > 1e-12 {
		imb = (buy - sell) / den
	}

	st.vol.Update(px)
	sigma := st.vol.Sigma()
	if sigma <= 0 {
		sigma = 1
	}

	return Squash(st.z.Update(imb/sigma), st.squash)
}

// --- Model C: Multi-EMA Power-Law ---

type MultiEMAConfig struct {
	TauSec, Weights                          []float64
	D0, VolLambda, ZScoreLambda, SquashScale float64
}

type MultiEMAState struct {
	cfg      MultiEMAConfig
	lastTsMs int64
	ema      []float64
	vol      VolEWMA
	z        ZScoreEWMA
}

func NewMultiEMAState(cfg MultiEMAConfig) *MultiEMAState {
	return &MultiEMAState{
		cfg: cfg,
		ema: make([]float64, len(cfg.TauSec)),
		vol: VolEWMA{Lambda: cfg.VolLambda},
		z:   ZScoreEWMA{Lambda: cfg.ZScoreLambda},
	}
}

func (st *MultiEMAState) Update(row AggRow) float64 {
	ts := row.TsMs
	d := TradeDollar(row)
	s := TradeSign(row)
	px := TradePrice(row)

	var dtSec float64
	if st.lastTsMs == 0 {
		st.lastTsMs = ts
		dtSec = 0
	} else {
		dtSec = float64(ts-st.lastTsMs) / 1000.0
		if dtSec < 0 {
			dtSec = 0
		}
		st.lastTsMs = ts
	}

	mark := 0.0
	if d > 0 && st.cfg.D0 > 0 {
		mark = math.Log(1.0 + d/st.cfg.D0)
	}
	x := s * mark

	for j, tau := range st.cfg.TauSec {
		if tau <= 0 {
			continue
		}
		if dtSec > 0 {
			lambda := math.Exp(-dtSec / tau)
			st.ema[j] = lambda*st.ema[j] + (1-lambda)*x
		} else {
			st.ema[j] = x
		}
	}

	imb := 0.0
	for j, w := range st.cfg.Weights {
		imb += w * st.ema[j]
	}

	st.vol.Update(px)
	sigma := st.vol.Sigma()
	if sigma <= 0 {
		sigma = 1
	}

	return Squash(st.z.Update(imb/sigma), st.cfg.SquashScale)
}

// --- Model D: EMA Baseline ---

type EMAConfig struct {
	TauSec, D0, VolLambda, ZScoreLambda, SquashScale float64
}

type EMAState struct {
	cfg      EMAConfig
	lastTsMs int64
	ema      float64
	vol      VolEWMA
	z        ZScoreEWMA
}

func NewEMAState(cfg EMAConfig) *EMAState {
	return &EMAState{
		cfg: cfg,
		vol: VolEWMA{Lambda: cfg.VolLambda},
		z:   ZScoreEWMA{Lambda: cfg.ZScoreLambda},
	}
}

func (st *EMAState) Update(row AggRow) float64 {
	ts := row.TsMs
	d := TradeDollar(row)
	s := TradeSign(row)
	px := TradePrice(row)

	var dtSec float64
	if st.lastTsMs == 0 {
		st.lastTsMs = ts
		dtSec = 0
	} else {
		dtSec = float64(ts-st.lastTsMs) / 1000.0
		if dtSec < 0 {
			dtSec = 0
		}
		st.lastTsMs = ts
	}

	mark := 0.0
	if d > 0 && st.cfg.D0 > 0 {
		mark = math.Log(1.0 + d/st.cfg.D0)
	}
	x := s * mark

	if dtSec > 0 && st.cfg.TauSec > 0 {
		lambda := math.Exp(-dtSec / st.cfg.TauSec)
		st.ema = lambda*st.ema + (1-lambda)*x
	} else {
		st.ema = x
	}

	st.vol.Update(px)
	sigma := st.vol.Sigma()
	if sigma <= 0 {
		sigma = 1
	}

	return Squash(st.z.Update(st.ema/sigma), st.cfg.SquashScale)
}
```

// --- End File: ofibuild.go ---

// --- File: ofistudy.go ---

```go
package main

import (
	"encoding/binary"
	"fmt"
	"io"
	"math"
	"os"
	"path/filepath"
	"slices"
	"sort"
	"strings"
	"sync"
	"text/tabwriter"
	"time"
)

// --- Study Config ---

const (
	OOSDateStr   = "2024-01-01"
	StudyThreads = 24         // Saturate all 24 logical cores on Ryzen 9 7900X
	StudyMaxRows = 10_000_000 // Per-thread default row budget for buffers
)

// TimeHorizonsSec: The horizons to test.
var TimeHorizonsSec = []int{10, 30, 60, 180, 300}

var oosBoundaryYMD int

func init() {
	oosBoundaryYMD = parseOOSBoundary(OOSDateStr)
}

// --- Data Structures ---

// Accumulator aggregates stats across days for a single horizon.
type Accumulator struct {
	Days      int
	SumIC     float64
	SumSharpe float64
	SumHit    float64
	SumBE     float64
}

// DayResult holds one day's stats for ALL variants and ALL horizons.
// Map key: VariantID -> []MetricStats (index corresponds to TimeHorizonsSec)
type DayResult struct {
	Stats map[string][]MetricStats
	YMD   int
}

// --- Main Runner ---

func runStudy() {
	startT := time.Now()
	featRoot := filepath.Join(BaseDir, "features", Symbol)

	// 1. Discover Variants
	entries, err := os.ReadDir(featRoot)
	if err != nil {
		fmt.Printf("[err] reading feature dir: %v\n", err)
		return
	}
	var variants []string
	for _, e := range entries {
		if e.IsDir() {
			variants = append(variants, e.Name())
		}
	}
	slices.Sort(variants)

	if len(variants) == 0 {
		fmt.Println("[warn] No variants found.")
		return
	}

	fmt.Printf("--- OFI STUDY | %s | %d Variants | Split: %s ---\n", Symbol, len(variants), OOSDateStr)
	fmt.Printf("[arch] Optimized Day-Parallel Pipeline (Ryzen 7900X)\n")

	// 2. Discover Common Days (intersection of Raw Data and Features)
	tasks := discoverStudyDays(filepath.Join(featRoot, variants[0]))
	if len(tasks) == 0 {
		fmt.Println("[warn] No .bin days found for first variant.")
		return
	}

	workerCount := StudyThreads
	if workerCount > CPUThreads {
		workerCount = CPUThreads
	}
	fmt.Printf("[job] Processing %d days using %d threads.\n", len(tasks), workerCount)

	// 3. Worker Pool (Day-Parallel)
	resultsChan := make(chan DayResult, len(tasks))
	jobsChan := make(chan int, len(tasks))
	var wg sync.WaitGroup

	// Accumulators: Variant -> Horizon -> Accumulator
	isAcc := make(map[string][]Accumulator)
	oosAcc := make(map[string][]Accumulator)

	for _, v := range variants {
		isAcc[v] = make([]Accumulator, len(TimeHorizonsSec))
		oosAcc[v] = make([]Accumulator, len(TimeHorizonsSec))
	}

	// Spin up workers
	for i := 0; i < workerCount; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()

			// --- Thread-Local Buffers (reused across days) ---
			maxRows := StudyMaxRows

			prices := make([]float64, maxRows)
			times := make([]int64, maxRows)
			sigBuf := make([]float64, maxRows)

			// Scratch buffers for per-horizon calculations
			scratchSig := make([]float64, 0, maxRows)
			scratchRet := make([]float64, 0, maxRows)

			// File read buffer (reused for signal files)
			fileBuf := make([]byte, maxRows*8)

			for idx := range jobsChan {
				dayInt := tasks[idx]
				res := processStudyDay(
					dayInt, variants, featRoot,
					prices, times, sigBuf, fileBuf,
					&scratchSig, &scratchRet,
				)
				resultsChan <- res
			}
		}()
	}

	// Enqueue jobs
	for i := range tasks {
		jobsChan <- i
	}
	close(jobsChan)

	// Close results channel when workers finish
	go func() {
		wg.Wait()
		close(resultsChan)
	}()

	// 4. Aggregation (Main Thread)
	for res := range resultsChan {
		if len(res.Stats) == 0 {
			continue
		}
		isOOS := res.YMD >= oosBoundaryYMD

		for v, hStats := range res.Stats {
			for hIdx, stat := range hStats {
				if stat.Count == 0 {
					continue
				}

				var target []Accumulator
				if isOOS {
					target = oosAcc[v]
				} else {
					target = isAcc[v]
				}

				target[hIdx].Days++
				target[hIdx].SumIC += stat.ICPearson
				target[hIdx].SumSharpe += stat.SharpeAnnual
				target[hIdx].SumHit += stat.HitRate
				target[hIdx].SumBE += stat.BreakevenBps
			}
		}
	}

	// 5. Reporting
	fmt.Println()
	for hIdx, sec := range TimeHorizonsSec {
		printHorizonTable(sec, variants, isAcc, oosAcc, hIdx)
		fmt.Println()
	}

	fmt.Printf("[study] Complete in %s\n", time.Since(startT))
}

// processStudyDay loads Raw Data ONCE, then iterates variants.
func processStudyDay(
	dayInt int,
	variants []string,
	featRoot string,
	prices []float64,
	times []int64,
	sigBuf []float64,
	fileBuf []byte,
	scratchSig, scratchRet *[]float64,
) DayResult {
	y := dayInt / 10000
	m := (dayInt % 10000) / 100
	d := dayInt % 100

	res := DayResult{
		YMD:   dayInt,
		Stats: make(map[string][]MetricStats),
	}

	// 1. Load Raw Data (Expensive ZLIB op - Done ONCE)
	rawBytes, rowCount, ok := loadRawDay(y, m, d)
	if !ok || rowCount == 0 {
		return res
	}

	n := int(rowCount)

	// --- CRITICAL FIX: Grow buffers instead of dropping high-vol days ---
	if n > cap(prices) {
		newCap := n + n/4 // 25% headroom
		prices = make([]float64, newCap)
		times = make([]int64, newCap)
		sigBuf = make([]float64, newCap)
	}
	if n > cap(*scratchSig) {
		*scratchSig = make([]float64, 0, n+n/4)
	}
	if n > cap(*scratchRet) {
		*scratchRet = make([]float64, 0, n+n/4)
	}
	if n*8 > cap(fileBuf) {
		fileBuf = make([]byte, n*8+n*2)
	}

	// Reslice working views to exactly n rows
	prices = prices[:n]
	times = times[:n]
	sigBuf = sigBuf[:n]

	// 2. Parse Raw Data (Vectorized parsing)
	for i := 0; i < n; i++ {
		off := i * RowSize // RowSize=48 defined in common.go
		// Prices at offset 8 (uint64), Times at offset 38 (uint64)
		pBits := binary.LittleEndian.Uint64(rawBytes[off+8:])
		tBits := binary.LittleEndian.Uint64(rawBytes[off+38:])

		prices[i] = float64(pBits) // PxScale is constant; relative moves are what matter
		times[i] = int64(tBits)
	}

	// 3. Iterate Variants
	dStr := fmt.Sprintf("%04d%02d%02d", y, m, d)

	for _, v := range variants {
		sigPath := filepath.Join(featRoot, v, dStr+".bin")

		// Fast Load Signals
		loadedSigs, ok := fastLoadFloats(sigPath, fileBuf, sigBuf)
		if !ok || len(loadedSigs) != n {
			continue
		}

		// Calc Stats for all horizons
		statsList := make([]MetricStats, len(TimeHorizonsSec))
		for hIdx, sec := range TimeHorizonsSec {
			statsList[hIdx] = calcDailyStatsTimePrepared(
				loadedSigs, prices, times, sec*1000, scratchSig, scratchRet,
			)
		}
		res.Stats[v] = statsList
	}

	return res
}

// fastLoadFloats reads binary floats into a pre-allocated buffer to reduce GC.
func fastLoadFloats(path string, fileBuf []byte, outBuf []float64) ([]float64, bool) {
	f, err := os.Open(path)
	if err != nil {
		return nil, false
	}
	defer f.Close()

	// Stat to get size
	fi, err := f.Stat()
	if err != nil {
		return nil, false
	}
	size := int(fi.Size())
	if size <= 0 || size%8 != 0 {
		return nil, false
	}
	count := size / 8

	if count > cap(outBuf) {
		// Should be extremely rare if StudyMaxRows is set appropriately.
		outBuf = make([]float64, count)
	} else {
		outBuf = outBuf[:count]
	}

	// Ensure fileBuf is large enough
	if cap(fileBuf) < size {
		fileBuf = make([]byte, size)
	}
	fileBuf = fileBuf[:size]

	// Single Syscall Read using io.ReadFull
	if _, err := io.ReadFull(f, fileBuf); err != nil {
		return nil, false
	}

	// Binary Decode
	for i := 0; i < count; i++ {
		bits := binary.LittleEndian.Uint64(fileBuf[i*8:])
		outBuf[i] = math.Float64frombits(bits)
	}

	return outBuf, true
}

// calcDailyStatsTimePrepared matches input logic but uses pointers for scratch buffers.
func calcDailyStatsTimePrepared(
	sig []float64,
	prices []float64,
	times []int64,
	horizonMs int,
	scratchSig *[]float64,
	scratchRet *[]float64,
) MetricStats {
	n := len(sig)
	if n < 200 || len(prices) != n || len(times) != n {
		return MetricStats{}
	}

	// Reset scratch length, keep capacity
	vSig := (*scratchSig)[:0]
	vRet := (*scratchRet)[:0]

	j := 0
	hVal := int64(horizonMs)

	for i := 0; i < n; i++ {
		pStart := prices[i]
		s := sig[i]

		// Fast reject: invalid price or zero signal
		if pStart <= 0 || s == 0 {
			continue
		}

		t0 := times[i]
		target := t0 + hVal
		if target <= t0 {
			continue
		}

		// Ensure j is strictly forward
		if j < i+1 {
			j = i + 1
		}
		for j < n && times[j] < target {
			j++
		}
		if j >= n {
			break // no more future horizon points
		}

		pEnd := prices[j]
		if pEnd <= 0 {
			continue
		}

		// simple return
		r := (pEnd - pStart) / pStart

		vSig = append(vSig, s)
		vRet = append(vRet, r)
	}

	*scratchSig = vSig
	*scratchRet = vRet

	if len(vSig) < 2 {
		return MetricStats{}
	}

	return ComputeStats(vSig, vRet)
}

// --- Helpers ---

func discoverStudyDays(vDir string) []int {
	var days []int
	files, _ := os.ReadDir(vDir)
	for _, f := range files {
		if strings.HasSuffix(f.Name(), ".bin") {
			name := strings.TrimSuffix(f.Name(), ".bin")
			if len(name) == 8 {
				val := fastAtoi(name)
				if val > 0 {
					days = append(days, val)
				}
			}
		}
	}
	sort.Ints(days)
	return days
}

func printHorizonTable(
	sec int,
	variants []string,
	isAcc, oosAcc map[string][]Accumulator,
	hIdx int,
) {
	// Build results list for sorting
	type row struct {
		Name string
		IS   Accumulator
		OOS  Accumulator
	}
	var rows []row

	for _, v := range variants {
		rows = append(rows, row{
			Name: v,
			IS:   isAcc[v][hIdx],
			OOS:  oosAcc[v][hIdx],
		})
	}

	// Sort by OOS Sharpe Descending
	slices.SortFunc(rows, func(a, b row) int {
		sa := 0.0
		if a.OOS.Days > 0 {
			sa = a.OOS.SumSharpe / float64(a.OOS.Days)
		}
		sb := 0.0
		if b.OOS.Days > 0 {
			sb = b.OOS.SumSharpe / float64(b.OOS.Days)
		}
		if sa > sb {
			return -1
		}
		if sa < sb {
			return 1
		}
		return 0
	})

	fmt.Printf("== Horizon %d seconds ==\n", sec)
	w := tabwriter.NewWriter(os.Stdout, 0, 0, 2, ' ', 0)
	fmt.Fprintln(w, "VARIANT\tIS_DAYS\tOOS_DAYS\tIS_IC\tOOS_IC\tIS_SR\tOOS_SR\tIS_HIT\tOOS_HIT\tIS_BE\tOOS_BE")
	fmt.Fprintln(w, "-------\t-------\t--------\t-----\t------\t-----\t------\t------\t-------\t-----\t------")

	for _, r := range rows {
		var isIC, isSR, isHit, isBE float64
		var oosIC, oosSR, oosHit, oosBE float64

		if r.IS.Days > 0 {
			div := float64(r.IS.Days)
			isIC = r.IS.SumIC / div
			isSR = r.IS.SumSharpe / div
			isHit = (r.IS.SumHit / div) * 100
			isBE = r.IS.SumBE / div
		}
		if r.OOS.Days > 0 {
			div := float64(r.OOS.Days)
			oosIC = r.OOS.SumIC / div
			oosSR = r.OOS.SumSharpe / div
			oosHit = (r.OOS.SumHit / div) * 100
			oosBE = r.OOS.SumBE / div
		}

		fmt.Fprintf(w, "%s\t%d\t%d\t%.4f\t%.4f\t%.2f\t%.2f\t%.1f%%\t%.1f%%\t%.1f\t%.1f\n",
			r.Name, r.IS.Days, r.OOS.Days,
			isIC, oosIC, isSR, oosSR, isHit, oosHit, isBE, oosBE,
		)
	}
	_ = w.Flush()
}

func parseOOSBoundary(dateStr string) int {
	if len(dateStr) < 10 {
		return 0
	}
	y := fastAtoi(dateStr[0:4])
	m := fastAtoi(dateStr[5:7])
	d := fastAtoi(dateStr[8:10])
	return y*10000 + m*100 + d
}

// fastAtoi converts an ASCII digit string (no sign) to int.
func fastAtoi(s string) int {
	n := 0
	for i := 0; i < len(s); i++ {
		c := s[i]
		if c < '0' || c > '9' {
			return 0
		}
		n = n*10 + int(c-'0')
	}
	return n
}
```

// --- End File: ofistudy.go ---

// --- File: sanity.go ---

```go
package main

import (
	"bytes"
	"compress/zlib"
	"crypto/sha256"
	"encoding/binary"
	"fmt"
	"io"
	"os"
	"path/filepath"
	"sync"
)

// runSanity verifies that all month-level data/index files are structurally sound
// and that checksums and basic header/length invariants hold.
func runSanity() {
	root := filepath.Join(BaseDir, Symbol)
	dirs, err := os.ReadDir(root)
	if err != nil {
		fmt.Printf("SANITY: cannot read root %s: %v\n", root, err)
		return
	}

	var tasks []string
	for _, y := range dirs {
		if !y.IsDir() {
			continue
		}
		months, err := os.ReadDir(filepath.Join(root, y.Name()))
		if err != nil {
			fmt.Printf("SANITY: cannot read year %s: %v\n", y.Name(), err)
			continue
		}
		for _, m := range months {
			if m.IsDir() {
				tasks = append(tasks, filepath.Join(root, y.Name(), m.Name()))
			}
		}
	}

	fmt.Printf("SANITY CHECK: %s (%d months)\n", Symbol, len(tasks))

	var wg sync.WaitGroup
	jobs := make(chan string, len(tasks))

	for i := 0; i < CPUThreads; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			for path := range jobs {
				validateMonth(path)
			}
		}()
	}

	for _, t := range tasks {
		jobs <- t
	}
	close(jobs)
	wg.Wait()
}

func validateMonth(dir string) {
	idxPath := filepath.Join(dir, "index.quantdev")
	dataPath := filepath.Join(dir, "data.quantdev")

	fIdx, err := os.Open(idxPath)
	if err != nil {
		fmt.Printf("FAIL: %s (No Index: %v)\n", dir, err)
		return
	}
	defer fIdx.Close()

	fData, err := os.Open(dataPath)
	if err != nil {
		fmt.Printf("FAIL: %s (No Data: %v)\n", dir, err)
		return
	}
	defer fData.Close()

	hdr := make([]byte, 16)
	if _, err := io.ReadFull(fIdx, hdr); err != nil {
		fmt.Printf("FAIL: %s (Index header read error: %v)\n", dir, err)
		return
	}

	if string(hdr[:4]) != IdxMagic {
		fmt.Printf("FAIL: %s (Bad index magic)\n", dir)
		return
	}

	count := binary.LittleEndian.Uint64(hdr[8:])
	row := make([]byte, 26)
	issues := 0

	for i := uint64(0); i < count; i++ {
		if _, err := io.ReadFull(fIdx, row); err != nil {
			fmt.Printf("FAIL: %s (Index row read error at %d: %v)\n", dir, i, err)
			issues++
			break
		}

		offset := int64(binary.LittleEndian.Uint64(row[2:]))
		length := int(binary.LittleEndian.Uint64(row[10:]))
		expSum := binary.LittleEndian.Uint64(row[18:])

		if length <= 0 {
			issues++
			continue
		}

		// Read compressed blob
		compData := make([]byte, length)
		if _, err := fData.Seek(offset, io.SeekStart); err != nil {
			issues++
			continue
		}
		if _, err := io.ReadFull(fData, compData); err != nil {
			issues++
			continue
		}

		r, err := zlib.NewReader(bytes.NewReader(compData))
		if err != nil {
			issues++
			continue
		}
		aggBlob, err := io.ReadAll(r)
		r.Close()
		if err != nil {
			issues++
			continue
		}

		// Check checksum
		s := sha256.Sum256(aggBlob)
		if binary.LittleEndian.Uint64(s[:8]) != expSum {
			issues++
			continue
		}

		// Basic structural checks
		if len(aggBlob) < HeaderSize {
			issues++
			continue
		}

		// Validate header magic
		if string(aggBlob[:4]) != AggMagic {
			issues++
			continue
		}

		rowCount := binary.LittleEndian.Uint64(aggBlob[8:])
		expectedLen := HeaderSize + int(rowCount)*RowSize
		if expectedLen != len(aggBlob) {
			issues++
			continue
		}
	}

	if issues > 0 {
		fmt.Printf("ISSUES: %s (%d errors)\n", dir, issues)
	}
}
```

// --- End File: sanity.go ---

